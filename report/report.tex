\documentclass[a4paper, 11pt]{article}

\usepackage{geometry}
\usepackage{layout}
\geometry{includeheadfoot, margin=3.18cm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array,booktabs}
\usepackage[style=numeric,sorting=debug,backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{todonotes}
\usepackage{paralist}
\usepackage{listings}
\usepackage{lmodern}
\usepackage{minted}
\usepackage[fleqn]{amsmath}
\usepackage{hyperref}
\usepackage[center]{caption}
\usepackage{multirow}
\usepackage{pbox}
\usepackage{arydshln}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}
\usemintedstyle{autumn}
\lstset{language=SQL}
\addbibresource{report.bib}
\DeclareDatamodelEntrytypes{standard}
\DeclareDatamodelEntryfields[standard]{type,number}
\DeclareBibliographyDriver{standard}{%
  \usebibmacro{bibindex}%
  \usebibmacro{begentry}%
  \usebibmacro{author}%
  \setunit{\labelnamepunct}\newblock
  \usebibmacro{title}%
  \newunit\newblock
  \printfield{number}%
  \setunit{\addspace}\newblock
  \printfield[parens]{type}%
  \newunit\newblock
  \usebibmacro{location+date}%
  \newunit\newblock
  \iftoggle{bbx:url}
    {\usebibmacro{url+urldate}}
    {}%
  \newunit\newblock
  \usebibmacro{addendum+pubstate}%
  \setunit{\bibpagerefpunct}\newblock
  \usebibmacro{pageref}%
  \newunit\newblock
  \usebibmacro{related}%
  \usebibmacro{finentry}}
% Stop Latex from repositioning tables like an idiot
\restylefloat{table}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the
% horizontal lines, change thickness here

\center

\textsc{\LARGE Imperial College London}\\[1.5cm]
\textsc{\Large Department of Computing}\\[0.5cm]
\textsc{\large 3rd Year Group Project Report}\\[0.5cm]

\HRule \\[0.4cm]
{\huge \bfseries Translation of}\\[0.4cm]
{\huge \bfseries First Order Predicate Logic to SQL}\\[0.4cm]% Title of your document
\HRule \\[1.5cm]

\begin{minipage}[t]{0.4\textwidth}
  \begin{flushleft} \large
    \emph{Authors:}\\
    Sean \textsc{Allan}\\
    Mitchell \textsc{Allison}\\
    Sam \textsc{Esgate}\\
    Thomas \textsc{Harling}\\
    Ted \textsc{Sales}\\
    Max \textsc{Tottenham}
  \end{flushleft}
\end{minipage}%
%
\begin{minipage}[t]{0.4\textwidth}
  \begin{flushright} \large
    \emph{Supervisor:} \\
    Dr. Fariba \textsc{Sadri}  % Supervisor's Name
  \end{flushright}
\end{minipage}%
\\[4cm]

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\renewcommand{\contentsname}{\huge Contents \vspace{1cm}}
\tableofcontents
\clearpage

\setlength{\parskip}{0.3cm} \setlength{\parindent}{0cm}

\section{Executive Summary}
  \todo[inline]{A. Executive Summary 5\% of the whole project!

    Your 'elevator pitch'

    What is your Project? What does it do? Why would I want to buy it? etc.

    No implementation, software engineering details, or project management 
}

  At its heart our project is a language translator which, for a given domain,
  takes a restricted version of first-order predicate logic as input and
  produces the corresponding SQL as output. This allows any SQL relational
  database to be queried using only first-order predicate logic, without the
  user needing to know any SQL.

  The predominant use-case for our project is as a teaching tool for use by
  students studying both logic and databases; topics which
  many universities deem mandatory for all computing students. To
  reinforce a student's understanding of logic, queries can be written and
  tested against a suitably populated database. The result of these queries
  can then be used to determine how well they have understood the course
  material, perhaps using an auto-checker.

  The generated SQL could be used as a reference for the student, either to
  highlight issues with their logic or to explain the formulation of correct
  SQL, providing them with extensive revision notes.

  The project also lends itself to creating problem sheets for these subjects.
  For example, a teacher could create a list of questions in english, for instance
  ``what are the names of dragons that are happy?'' requiring the student to
  give both the predicate logic and the SQL yielding the answer to the
  question. Our project could then be used to easily check the student's work.

  Also academics who are more used to writing logic can save time and not have
  to know SQL implementation details. Also server side improvements can be added
  without the user's knowledge to increase performance or optimise queries. (by
  improving the sql generated by the parser, or using new sql commands when
  database makers release updates).

  Can allow queries to be run on the database whilst exposing less information
  about it - don't show set users the database structure. Could use predefined
  functions (doubt we do this at the moment) to do things like a 'black box'.

  In summary, we have created a system that creates SQL queries from logic 
  statements. This allows you to query a database without knowing anything
  about SQL and importantly to ignore which variant of SQL you
  are using and get on with extracting data. It provides the basis for a
  unique teaching tool for both SQL and logic and it brings some power
  of logic to regular databases, without the need to migrate to datalog.

\section{Introduction}

  \todo{Set the scene ... motivation'
  
  State the problem you are trying to solve ...objective(s)

  Summarise your main achievements}

When we began this project our goal was simply to allow a database to be 
queried using logic. You shouldn't need to know the syntax of a particular
version of SQL, you shouldn't have to explicitly tell the DBMS how to extract
the data, all you need to do is write a query in logic and we will
get it for you.

We wanted to make it as easy as possible to write queries and view results;
our objective was to make it easy to understand, easy to write and to
provide clear advice when it does go wrong.

At the end of the project we can say that we have achieved most of our initial 
goals. We have created a system that can convert the queries, we have created
a simple way to construct and edit these queries, one that can be changed to 
work on any database and provide clear error messages.

  -- GOALS - In Chatley coursework 1 --
  Unfortunately there aren't any goals in the chatley coursework one.
  Just a bunch of questions, turns out we forgot to add them. WHOOPS LOOL
  TODO: Discuss goals, they're referenced in the Project Management
  section. Revisions should not be talked about here though, just the
  overarching goals of the project.

\section{Background Research}
\label{sec:background}
\subsection{SQL}
  Structured Query Language (SQL)~\cite{wiki:SQL} is a special-purpose
  programming language designed for managing data held in relational database
  management system. SQL is made up of two different types of language:

  \begin{itemize}
      \item
        Data Definition Language. DDL is used to define the structure of a
        database (also known as its schema).
      \item
        Data Manipulation Language. DML is primarily used to insert, select,
        delete and update data within a database.
  \end{itemize}

  However, as defined by the SQL92 standard\cite{isoSQL}, read-only operations
  such as 'SELECT' (without 'INSERT INTO') should not exist as part of the DML;
  they do not manipulate the data, only query it. This distinction between
  read-only and read-write operations is not enforced, but here we will focus
  only on read-only operations (so called 'SQL-data' operations).

  We initially chose to restrict to these operations so we could build a basic,
  but fully working system before progressing. Afterwards we considered how
  a statement that manipulates data, like an insert, could be defined in logic.
  \begin{gather}
    \top \rightarrow (film\_title(\$new, "Flight") \land film\_director(\$new, "James"))
  \end{gather}

  Above is one possibility, inserting a film called 'Flight', directed by 
  'James'. Having $\top$ imply the new information suggests that it must be 
  true, and if it doesn't currently hold it should be inserted. Problems here
  include automatically generating an id whilst keeping the syntax as true to
  logic as possible. Given time it would be possible develop this but for now
  it remains unimplemented.

\subsection{Tuple Relational Calculus}
  SQL was originally based on Relational Algebra and Tuple Relational
  Calculus. RA forms the structure and operations that can be performed
  across tuples of data and TRC provides a query language for such a model.

  \subsubsection{Formal Specification of Tuple Relational Calculus\cite{lecRA}}
    \label{sec:formalTRC}
    A query takes the form: \{T | p(T)\}

    The answer is the set of all tuples T such that p(T) evaluates to true.

    A formula F is recursively defined as:
    \begin{itemize}
      \item $p(t_1, ..., t_n)$ where predicate $p$ is applied to terms $t_1, ..., t_n$ (atomic formula)
        \item $\lnot F$ where $F$ is a formula.
        \item $F' \land F''$ where $F', F''$ are formulae.
        \item $F' \lor F''$ where $F', F''$ are formulae.
        \item $\exists t(F)$ where $F$ is a formula and $t$ is a tuple of terms.
        \item $\forall t(F)$ where $F$ is a formula and $t$ is a tuple of terms.
      \end{itemize}

      $\exists t(F)$ is true if, for some tuple $t$ the formula $F$ is true. \\
      $\forall t(F)$ is true if, for all tuples $t$ the formula $F$ is true.

      A variable $v$ is bound in $F$ if it is of the form:

      $\exists t(F)$ or $\forall t(F)$ and $v \in t$.

      Otherwise $v$ is said to be free.
      
      Note that in query \{T | p(T)\} the variables in T must be the only
      free variables in p(T).

    \subsubsection{Querying Relational Algebra}
      Using all of this, it is possible to begin formulating some simple
      queries. For example, suppose it was desirable to query all films from a
      particular director in a database. It would be simple to write:

      \begin{center}
        \{F | F $\in$ Films $\land$ F.director = "Matt Damon"\}
      \end{center}

      This would however return all the elements of the tuple. The tuple may
      contain many irrelevant values to the user, and so it would be desirable
      to only return the title of said film. This could be achieved as detailed
      below:

      \begin{center}
        \{F | $\exists$F1 $\in$ Films(F1.director = "Matt Damon" $\land$ 
          F.title = F1.title)\}
      
        \emph{NOTE: F becomes a tuple with one element called title.}
      \end{center}

      We are now able to represent projection and selection. We can also
      represent joins. Let us suppose that we wish to find all titles of films
      whose director has also directed another film.

      \begin{center}
        \{F | $\exists$F1 $\in$ Films($\exists$F2 $\in$ 
        Films(F1.director = F2.director $\land$ F1 != F2)
        \\ $\land$ F.title = F1.title)\}
      \end{center}

\subsection{Mapping Tuple Relational Calculus to First-order Predicate Logic}
    With the knowledge of TRC, and how it underpins SQL, it is now necessary to
    formulate a mapping between TRC and first order predicate logic.

    \subsubsection{Atoms, Formulae, Predicates}
      Firstly atoms, formulae and predicates remain unchanged. We still wish to
      find a set of tuples that will result in a given formula evaluating to
      true.

    \subsubsection{Set Membership}
      Before we could simply test the membership of a tuple $t$ in a
      relation $R$ with $t \in R$. First order predicate logic, at least it's basic,
      unsorted form, does not have the notion of variables belonging to
      different types or sets, they all belong to one global set. To solve
      this problem we propose a few solutions.

      One solution is with an 'InRelation' predicate, where 'InRelation(t, R)'
      would have the same semantic meaning as the previous set membership test.
      This seems to work, although throughout first order predicate logic, we
      do not have the notion of a tuple; an atom is structurally meaningless. A
      disadvantage of this would be that accessing members of the tuple would
      require another predicate, for example 'InTuple(a, 'attrName', t)' which
      would have the same semantic meaning as 't.a' in TRC.

      Another solution, which solves the previous issue, is to generate n-ary
      predicates that represent a tuple. For example, given a relation 'films',
      with columns 'title', 'length' and 'date-released', a predicate
      'films(title, length, date-released)' could be generated, where title,
      length and date-released are all variables. This certainly solves the
      issue the set membership issue, but does add uneccessary bloat to each
      query; relations may have thousands of columns, very few of which are
      likely needed in a given query.

      The solution chosen addresses the issues raised so far.

      For a relation R(k, $a_{1}$, $a_{2}$, ..., $a_{n}$) with primary key k
      and n attributes we generate n + 1 predicates:
      \begin{align*}
        \text{1:  }     & R(k)           \\
        \text{2:  }     & R.a_1(k, a1)   \\
        \text{3:  }     & R.a_2(k, a2)   \\
                        & \vdots         \\
        \text{n + 1:  }  & R.a_n(k, an)
      \end{align*}
      NOTE: The predicate R.k(k, k) is also valid, albeit redundant.

      Relating a tuple to its primary key gives a concise way to address it,
      adding the slight restriction that each relation must have a primary key.

    \subsubsection{Free and Bound Variables}
    \label{sec:freebound}

      Looking back to Tuple Relational Calculus in
      section~\ref{sec:formalTRC}, we will use similar semantics for free and
      bound variables. Recall the syntax for the return value in TRC, $T$ in the
      query below

      \{T | p(T)\}

      That is to say that TRC returns the set of all tuples $T$, such that 
      $p(T) \models \top$. Without extending predicate logic, we cannot use 
      a similar format for specifying what is returned. However, all 
      variables that are free in $p(T)$ must be in $T$ and so we assume 
      that all unbound variables in the query are to be returned.

  \section{Translation of First-order Predicate Logic to SQL}

    Section~\ref{sec:background} provides a rough translation between
    first-order predicate logic and tuple relational calculus, and from this
    grounding we can begin to define the mapping between logic and SQL.

    The examples in the section below will make use of the filmdb as used in
    the project. A description of the SQL schema can be found in the appendix.

    \subsection{Quantifiers}

    \subsubsection{Existentially Quantified Queries}

    Suppose we have the following first-order predicate logic query.
    \begin{gather}
      \exists x(films.title(x, title)) \label{select1}
    \end{gather}
    The query binds $x$ and leaves $title$ free, meaning that our
    answer will be the set of all subtitutions for $title$ such that
    $\exists x(films.title(x, title)) \models \top$. The '$\exists$' symbol
    serves two purposes. Firstly it binds $x$ as mentioned above, and secondly
    it enforces that the substituted $title$ exists in at least one tuple.
    The predicate $films.title$ gives the translator the scope of
    relations to query (in this case, just the $films$ relation).

    We can translate the query with use of the SQL SELECT statement. The SELECT
    statement is used to select data from a database.\cite{w3SELECT} Suppose
    that we wish to query the database for every film title in the database.
    Below is an example of an SQL query that satisfies this criteria.

    \begin{minted}{sql}

    SELECT title
    FROM films;

    \end{minted}

    Strictly speaking since tuple relational calculus is based on set
    operators, like relational algebra, and SQL implcitly uses bag operators
    this actually evaluates to the following:

    \begin{minted}{sql}

    SELECT DISTINCT title
    FROM films;

    \end{minted}

    Note that it is perfectly valid to create a query with no bound variables,
    such as:
    \begin{gather}
      films.title(x, title) \label{select2}
    \end{gather}
    This will return the set of all tuples of the form \{$films.fid,
    films.title$\}, using the following SQL query.

    \begin{minted}{sql}

    SELECT fid, title,
    FROM films;

    \end{minted}

    Note that the DISTINCT keyword is no longer needed, as $films.fid$ is a
    primary key.

    Now we have demonstrated projecting a primary key, and a single attribute,
    all that is left is to project multiple attributes from a tuple. This
    requires the conjunction of multiple predicates, each over the same
    relation and using the same primary key (that is to say that each predicate
    relates to a single tuple). Suppose we wish to project
    every film title, and its corresponding length.

    \begin{gather}
      \exists x(films.title(x, y) \land films.length(x, z)
    \end{gather}

    As both $y$ and $z$ are from the same unique tuple identified by $x$
    (enforced by the conjunction within the query) our
    answer is equivalent to that returned by the following SQL query.

    \begin{minted}{sql}
    SELECT name, length
    FROM films;
    \end{minted}

    This can be extended to any number of predicates, although the relation and
    the key must be the same. If this is not the case, the query may represent
    a join, later described in section~\ref{sec:joins}.

    \subsubsection{Existentially Quantified Subqueries}
    \label{sec:existential}
    Often in a logical query, it will be common to existentially quantify a
    variable in an inner subquery. For example, the following subquery checks
    the existence of any films within our database with the same name, but that
    are not the same film, and returns the name of said film.
    \begin{gather}
      \exists x(films.title(x, title) \land \exists y(films.title(y,
      title) \land x != y))
    \end{gather}
    With a query of this format, we can simply translate the subquery, wrap it
    in an SQL EXISTS condition and conjunct this onto the WHERE clause of the
    outer query's query.

    \begin{minted}{sql}
      SELECT films1.title
      FROM films AS films1
      WHERE EXISTS (
        SELECT films2.title
        FROM films AS films2
        WHERE films1.fid <> films2.fid
      );
    \end{minted}

    There are two things to note with this approach, both relating to scope.
    The first is that the tables need be aliased. SQL requires that aliases
    in subqueries must not have been used previously as the outer aliases are
    still in scope.
    
    The second is that in our logic $y$ is bound and we cannot make it unbound
    without changing the meaning of the query. If we cannot make it unbound
    then we cannot return it. Interestingly, in the SQL the outer scope cannot
    access fields in the inner scope, and so similarly cannot return it.
    Therefore we must create an exception to the free and bound variable rules
    that we specify in section~\ref{sec:freebound}.

    We could also repesent the previous query in a simpler form.
    \begin{gather}
      \exists x,y(films.title(x, title) \land films.title(y, title)
        \land x != y)
    \end{gather}
    This would translate to the following SQL query.

    \begin{minted}{sql}
    SELECT films1.title
    FROM films as films1 JOIN films as films2 ON films1.title = films2.title
    WHERE films1.fid <> films2.fid;
    \end{minted}

    Each of these forms are equivalent, and will return the same result.
    However, it is useful to be able to represent nested quantifiers, as it
    will not always be equivalent to combine them, for example in the case
    of implication, negation or duplicate variable names.\cite{washEQUIV}

    \subsubsection{Universally Quantified Variables}
    Using our knowledge from section~\ref{sec:existential}, we can now
    formulate a translation for universally quantified variables. In
    first-order predicate logic, the following equivalence holds:
    \begin{multline}
      \forall x(P(x)) \equiv \lnot \exists x(\lnot P(x))
    \end{multline}
    We can use this equivalence to simply encounter universally quantified
    variables as existentially quantified. Below is an example query in logic
    that aims to return all directors such that every film that they have
    directed is longer than one hour.
    \begin{multline}
      \exists x(films.director(x, director) \land \forall y(films.director(y,
      director) \land films.length(y, length) \land length > '1:00:00'))
    \end{multline}
    We can think of this equivalently as following.
    \begin{multline}
      \label{eqn:except}
      \exists x(films.director(x, director) \land \lnot \exists y,length(\lnot(films.director(y,
      director) \land films.length(y, length) \land length > '1:00:00')))
    \end{multline}

    Using De Morgan's laws, we can infer the following
    equivalence.
    \begin{multline}
      \label{eqn:demorganequiv}
      \lnot(films.director(y, director) \land films.length(y, length) 
      \land length > '1:00:00') \equiv (films.director(y,
      director) \lor films.length(y, length) \lor length > '1:00:00')
    \end{multline}
    Equation~\ref{eqn:demorganequiv} however does not offer much progress, as
    we have not yet specified how to evaluate disjunction.

    Instead we must look to the query's semantic meaning. Referring to
    section~\ref{sec:except}, it can be seen that we need to make use of the
    SQL EXCEPT operator.

    We also must make use of the SQL EXISTS condition. The condition
    is used in a SQL query and is considered "to be met" if the subquery
    returns at least one row.\cite{technetEXISTS} The condition is useful in
    translating predicate logic to SQL, as it's meaning is consistent across
    both languages. 

    Our logical query now can be translated to the following SQL query.
    \begin{minted}{sql}
    SELECT film1.director
    FROM films AS films1
    WHERE NOT EXISTS (
      SELECT films2.director
      FROM films AS films2
      EXCEPT
      SELECT films2.director
      FROM films AS films2
      WHERE films2.length > '1:00:00' AND films2.director = films1.director);
    \end{minted}

    \subsection{Equality}

      Suppose that we wish to check the existence of a film (The Bourne
      Identity) within our database. Below is an SQL query that satisfies this
      criteria.

      The WHERE clause is used to extract only those records that fulfill a
      specified criterion.~\cite{w3WHERE} 


      \begin{minted}{sql}

      SELECT title,
      FROM films
      WHERE title = 'The Bourne Identity';

      \end{minted}

      We expect to encounter at least one tuple if the film exists in our domain
      of discourse. \todo{Cite this from our AD lectures} This can be represented
      in one of two ways in first-order predicate logic.
      \begin{gather}
        \exists x(films.title(x, y) \land y = \text{'The Bourne
        Identity'})\label{where1}\\
        films.title(x, \text{'The Bourne Identity'})\label{where2}
      \end{gather}
      Both of these queries are valid, however note that they do return different
      tuples. (\ref{where1}) returns the same answer as the SQL query above, although
      it is verbose. (\ref{where2}) returns the singleton tuple \{$films.fid$\},
      because under our definition, only free variables are returned. Most of
      the time this is acceptable, as it is often undesirable to return a
      constant term in the results, and also has the benefit of being more
      compact.

      So far we have used the notion of equality to bind variables to specific
      ground terms. We can also use equality to ensure that two variables have
      the same value. For example, consider the following logical query.
      \begin{gather}
        \exists a,b,y(actor.name(a, x) \land film.director(b, y) \land x = b)
      \end{gather}
      The query returns names of all of the actors who have also directed a
      film. This translates to the following SQL query. For now, do not dwell
      on the cross join, as this will be explained in further detail in
      section~\ref{sec:joins}.

      \begin{minted}{sql}
      SELECT name
      FROM actors CROSS JOIN film
      WHERE name = director;
      \end{minted}

      Note, that it is also possible to write the logic query in a more succint
      manner, as follows.
      \begin{gather}
        \exists a,b(actor.name(a, name) \land film.director(b, name))
      \end{gather}
      Here we have removed the equality constraint, instead unifying both
      predicates to the same variable. This is often a more desirable way to
      specify logical queries.

    \subsection{Inequality}

      Suppose that we wish to check the inverse of the previous section, that
      there exists other films except 'The Bourne Identity'. In SQL, this
      simply translates to the following.

      \begin{minted}{sql}
        SELECT title,
        FROM films
        WHERE title <> 'The Bourne Identity';
      \end{minted}

      The existence of one or more tuples satisfies our query; if no queries
      are returned, it must be the case that there are no other films except
      'The Bourne Identity'.

      As before, there are two ways we can represent the SQL query above in
      logic.
      \begin{gather}
        \exists x(films.title(x, y) \land y !=  \text{'The Bourne
        Identity'})\label{where3}\\
        \lnot films.title(x, \text{'The Bourne Identity'})\label{where4}
      \end{gather}
      The first query matches the output of our equivalent SQL query, but again
      is more verbose. The second returns the set of singleton tuples
      \{$films.fid$\} such that the title associated with that particular
      primary key is not 'The Bourne Identity'. This is not equivalent to our
      SQL expression however, and may be undesirable to use in this
      circumstance (it is likely that we would like to return all other film
      titles).

      \todo{Do we use <= or $\le$}

      We can also express other inequalities, such as '<', '<=', '>', '>='.
      Here we query for all the fid of every films with length greater than or
      equal to two hours.

      \begin{minted}{sql}
      SELECT fid,
      FROM films,
      WHERE length >= '2:00:00';
      \end{minted}

      This can be expressed using the following predicate logic query.
      \begin{gather}
        \exists x(films.length(x, y) \land x >= \text{'2:00:00'}) \label{where5}
      \end{gather}

      Queries can be constructed for the other inequalities in a
      similar manner.

    \subsection{Logical Connectives}

      It is often desirable to project a set of attributes from a statement
      made up of conditions and logical connectives. For example, suppose we
      wished to query the database for all film titles where the film is between 
      two and three hours in length. The relevant SQL query would take one of
      the following forms.

      \begin{minted}{sql}
      SELECT title
      FROM films
      WHERE films.length >= '2:00:00'
        AND films.length <= '3:00:00';

      SELECT title
      FROM films
      WHERE films.length BETWEEN '2:00:00' AND '3:00:00';
      \end{minted}

      This can simply be represented in first-order predicate logic with the
      following query.

      \begin{gather}
        \exists x, length(films.title(x, title) \land films.length(x,
        length)\\
        \land length >= '2:00:00' \land length <= '3:00:00')
      \end{gather}

      Notice how the last two constraints in the logical query translate
      with very little effort. The same applies to constraints seperated with
      an $\lor$ which is equivalent to OR in SQL.

  \subsection{Conjunction of Predicates}
    \label{sec:joins}

    It is often desirable to use data spread across many relations in order to
    form useful queries. We can think of this simply in first-order predicate
    logic as the conjunction of predicates. When translating to SQL, retreiving
    data from multiple relation requires joins.

    An SQL JOIN clause is used to combine rows from two or more tables, based
    on a common field between them.~\cite{w3JOINS} Joins can enable incredibly
    powerful queries, especially where information
    is normalized (split across many relations) to minimise duplication.

    \subsubsection{SQL CROSS JOIN}

      A cross join is equivalent to the cartesian product of two sets. Suppose
      that we wished to write a query that returned every possible pair of
      film title and actor name in the database. In ANSI-89 SQL, we could use 
      the following query.

      \begin{minted}{sql}
      SELECT title, name
      FROM films, actors;
      \end{minted}

      Alternatively, in ANSI-92 SQL we can explicitly write:

      \begin{minted}{sql}
      SELECT title, name
      FROM films CROSS JOIN actors;
      \end{minted}

      We can think of this query as the conjunction of two disjoint relations;
      the relations do not share a key, or if they do we are not using them as
      part of the join.

      \begin{gather}
        \exists fid, aid(films.title(fid, title) \land actors.name(aid, name))
      \end{gather}

      The example used is unlikely to be a useful query on the database. Cross
      joins are rarely used in this format. Instead, they are often combined
      with a WHERE clause to extract useful information. For example, if we
      wished to query every actor that acted in the film 'The Bourne Identity',
      we could could use the following logic query.

      \begin{gather}
        \exists fid(films.title(fid, \text{'The Bourne Identity'}) \nonumber\\
        \land actors.fid(aid, fid) \land actors.name(aid, name))
      \end{gather}

      Which specifies that we only wish to return the set of all singleton
      tuples \{$name$\} such that the actor starred in 'The Bourne Identity'.
      With our knowledge of SQL CROSS JOIN, we can express this as the
      following SQL query.

      \begin{minted}{sql}
      SELECT name
      FROM films CROSS JOIN actors
      WHERE films.fid = actors.fid AND films.title = 'The Bourne Identity';
      \end{minted}

      So far we have not discussed SQL aliases. Aliases are required if joining
      two instances of the same relation. To demonstrate this, suppose that we
      wish to generate the set of all pairs of actors that have appeared in the
      same film. This is achieved with the following SQL query.

      \begin{minted}{sql}
      SELECT a1.name,
             a2.name
      FROM films JOIN actors AS a1
        JOIN actors AS a2
      WHERE films.fid = a1.fid AND films.fid = a2.fid 
        AND a1.name != a2.name
      \end{minted}

      Notice that we cannot simply refer to the relation $actors$ anymore, as
      there are two of them; without an alias, the identifier would be
      ambiguous. Below is a representation of the query in logic.

      \begin{align}
        \label{largeCROSS}
          \exists fid, & a1, a2(  \nonumber\\
                       & films(fid) \land \nonumber\\
                       & actors.fid(a1, fid) \land actors.fid(a2, fid) \land \nonumber\\
                       & actors.name(a1, name1) \land actors.name(a2, name2) \land \nonumber\\
                       & name1 != name2 \nonumber\\
        ) \hspace{6mm} &  % Ignore horrendous bodge...
      \end{align}

      \todo{Actor doesn't have an fid field. Did we do this so we wouldn't have to mess about with casting?}

    \subsubsection{SQL INNER JOIN}

      Introduced in ANSI-92 SQL, INNER JOIN allows for a more readable version of
      the above CROSS JOIN, WHERE combination. The logical query~\ref{largeCROSS}
      can be written using the SQL INNER JOIN syntax as:

      \begin{minted}{sql}
      SELECT a1.name,
             a2.name
      FROM films CROSS JOIN actors AS a1
        CROSS JOIN actors AS a2
      ON films.fid = a1.fid AND films.fid = a2.fid 
        AND a1.name != a2.name
      \end{minted}

      INNER JOIN is the same as JOIN as of ANSI-92 SQL, and it the existence of
      the ON clause is not necessary (making it a cross join).

    \subsubsection{Translating to the Correct Join}

      Fundamentally, joining two relations often occurs as the result of a
      conjunction of predicates when translating from first-order predicate
      logic. However, there are a few different circumstances which affect the
      type of join that must be used. Figure~\ref{table:jointranslation} explains
      the different types of join that can result from a query.

      \newcommand*{\tabbox}[2][t]{%
            \centering\vspace{0pt}\parbox[#1][3.7\baselineskip]{6.5cm}{\strut#2\strut}}

      \begin{table}
        \label{table:jointranslation}
        \begin{tabular}{ | 
          p{\dimexpr 0.5\linewidth-2\tabcolsep} |
          p{\dimexpr 0.5\linewidth-2\tabcolsep} |
        }
        
        \hline
        Dilemma & Explanation \\
        \hline

        \tabbox{\centering $rel.attr1(k, a) \land rel.attr2(k, b)$ \\ becomes \\  No Join} 
        & 
        Both keys and relations are equal. This conjunction involves projecting
        two elements from the same tuple, and so no join is necessary.
        \\\hline
        
        \tabbox{\centering $rel1.attr1(k1, a) \land rel2.attr2(k2, b)$ \\ becomes \\ CROSS JOIN} 
        & 
        The keys are different and the relations may or may not be equal. Since
        we assume that a != b, there are no common elements, and a cross join
        must occur.
        \\\hline
        
        \tabbox{\centering $rel1.attr1(k1, a) \land rel2.attr2(k2, a)$ \\ becomes \\ JOIN ON \\ rel1.attr1 =
        rel2.attr2} 
        & 
        The keys are different, but the attributes rel1.attr1 and rel2.attr2
        are equal, thus we must join on these elements. Note that aliases are
        used to avoid ambiguity between the two sets of attributes if they
        overlap.
        \\\hline
        
        \tabbox{\centering $rel1.attr1(k, a) \land rel2.attr2(k, b)$ \\ becomes \\ JOIN ON \\ rel1.key =
        rel2.key}
        & 
        The keys are the same and we assume the attributes rel1.attr1 and rel2.attr2
        are different. We must join on the keys. Note that aliases are
        used to avoid ambiguity between the two sets of attributes if they
        overlap.
        \\\hline
        
      \end{tabular}
        \caption{How logic relates to joins. Rules can combine to create more
        complex joins.}
    \end{table}

  \subsection{Negation}

    \subsubsection{Difference}

    It may be the case that we need to negate a set of predicates. By example,
    we will refer to the subquery within
    equation~\ref{eqn:except}:
    \begin{multline}
    \exists y(\lnot(films.director(y,
      director) \land films.length(y, length) \land length > '1:00:00'))
    \end{multline}
    Semantically, we wish to prove that a y exists, such that y is the films
    fid, and the length of that film is above one hour. We could formulate the
    translation as below:
    \begin{minted}{sql}
    SELECT films.director
    FROM films
    WHERE length <= '1:00:00';
    \end{minted}
    So the answer is simply achieved by negating the condition. However, if
    there are other constraints in the query that create a join, we do not wish
    to negate these; we only wish to negate the conditions that restrict the
    data after a join. The simplest method here to solve this problem is to use
    the EXCEPT operator.

    The SQL EXCEPT operator takes the distinct rows of one query and returns
    the rows that do not appear in a second result set.\cite{wiki:EXCEPT}. It
    has the same semantic meaning as the set difference operator. The
    above SQL query is equivalent to the following.

    \begin{minted}{sql}
    SELECT films.director
    FROM films
    EXCEPT
    SELECT films.director
    FROM films
    WHERE length > '1:00:00';
    \end{minted}

  \subsubsection{NULL Values}
    The SQL language implements three-valued logic. A condition can either
    translate to TRUE, FALSE or UNKNOWN.

    Often in relations, it will be common to come across a NULL value. A NULL
    value can represent any of the following\cite{pmbNULL}:

    \begin{itemize}
      \item NULL represents a something that is not present in the UoD
      \item NULL represents something that might be present in the UoD, but
        we do not know its value at present
      \item NULL represents something that is present in the UoD, but we do
        not know its value at present
    \end{itemize}

    In our implementation, we will take NULL to be the second representation.

    This is particularly useful because it allows us to postulate questions like
    the following "List the names of all films who's director is not known" as:

    \begin{multline}
      \exists x(films.name(x,name) \land \lnot films.director(x, director) )
    \end{multline} 

    It is common for SQL queries to check if a value is NULL, as the user may
    wish to exclude these values, or perform a special case if they are
    encountered.

\section{Design and Implementation}
  Anandha: Summarise key implementation details (how did you do it? what technology was
  used

  %\todo{and why? what other technology was considered, but not used and why? Any technical challenges encountered and how addressed?  Any risks anticipated, and how mitigated.}

  \subsection{Application Type}
  We wanted a system that was easy to set up and worked on any operating system. The first point of discussion was whether a desktop client or a web application would be better suited.

  \begin{center}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabular}{ r | l | l }
      \multicolumn{1}{c |}{\textbf{Feature}} &\multicolumn{1}{c |}{\textbf{Desktop Client}} & \multicolumn{1}{c}{\textbf{Web Based}} \\
        \hline
        \emph{Processing}      & \ding{51} Local (Low Latency)   & \ding{55} Remote (High Latency) \\
        \emph{Interface}       & \ding{51} Native                & \ding{51} Universal \\
        \emph{Multi-Platform}  & \ding{55} Difficult             & \ding{51} Easy \\
        \emph{Installation}    & \ding{55} Required              & \ding{51} None \\
        \emph{Updates}         & \ding{55} Manual                & \ding{51} Automatic \\
    \end{tabular}
  \end{center}

  It is worth noting that using C++ and Qt would have mitigated some of the
  problems with desktop clients. Qt is a development framework that allows
  you to develop for Windows, Mac OS X and Linux simultaneously. This was
  appealing because we didn't want to limit ourselves to one platform but at
  the same time didn't have the time to develop three separate versions. Even
  taking all of this into account we felt that a web based system made more 
  sense.

  \subsection{Language Choice}
  We'd clearly need to develop a front-end, using the traditional combination
  of HTML, CSS and JavaScript, but what to choose for the backend? We didn't
  want anything complicated, just a simple web server to accept requests and
  a way of sending these requests on for processing. Using something like C
  for this was overkill, we wanted to be able to quickly prototype and 
  experiment - given this, three languages came to mind.
  
  \begin{center}
    \renewcommand{\arraystretch}{1.5}%
    \begin{tabular}{r | l | l | l}
      \multicolumn{1}{c |}{\textbf{Feature}} & \multicolumn{1}{c |}{\textbf{Python}} & \multicolumn{1}{c |}{\textbf{Java}} & \multicolumn{1}{c}{\textbf{Ruby}}\\
        \hline
        \emph{Complexity of Language} & \ding{51} Simple      & \ding{55} Average   & \ding{51} Simple \\
        \emph{Previous Experience}    & \ding{55} None        & \ding{51} Lots      & \textbf{-} Some \\
        \emph{Plugin Support}         & \ding{51} Good        & \ding{51} Good      & \ding{51} Good \\
        \emph{Compilation (Speed)}    & \ding{51} Interpreted & \ding{55} Compiled  & \ding{51} Interpreted \\
    \end{tabular}
  \end{center}

  Java was the first to be eliminated, even though all group members had
  experience using it, mainly because development takes longer when compared 
  with the others. Though it's much easier to write than say C, it lacks 
  features like dynamic typing which make rapid prototyping that much easier.

  There was litle to choose between Python and Ruby, both are garbage
  collected, dynamically typed and concise languages. We had used Ruby
  in our previous work and it was decided, simply out of curiosity,
  that we would try Python.
  
  \subsection{Front End}
    \subsubsection{Web server}
      The web.py framework~\cite{webpy} was chosen to host the
      user facing site. Doing so allowed us to quickly set up a functioning site
      and concentrate on developing useful features, instead of spending a large
      amount of time setting up a more complicated web server some other
      alternatives we considered (Django) appeared to have a more complex
      configuration.

      The JSON data format was used to transfer the logic input by the user to the
      server and the translated SQL to the user from the server. The format was
      chosen as it was very easy to parse by Javascript running in the user's
      browser and was well documented online due to its wide existing use.

    \subsubsection{Webpage plugins and libraries}
      The site made heavy use of jQuery, which greatly simplified the handling
      of JSON data sent between the user and the server.

    \subsubsection{Graphical user interface}
      \begin{figure}[h!]
        \includegraphics[width=\textwidth]{images/site_query.png}
        \caption{Main `Query' page}
      \end{figure}

      The interface that the user would see and interact with had to be simple
      and intuitive to use. In particular, this meant that logic queries had to
      be easy to input and the resulting SQL and the result of running it on the
      database had to be easy to read and understand.

      The javascript Ace~\cite{aceEditor} code editor was used for inputting the
      user's logic query. The editor was both extremely powerful and very easy
      to implement. The editor provided a familiar interface when compared with
      desktop applications such as Vim or Gedit.

      The editor had a number of advantages over using a simple text box form.
      This included syntax highlighting, line numbers, displaying hidden
      characters and providing many different themes.

      Syntax highlighting was achieved by \todo{Ted magic}. Syntax that the
      highlighting was able to distinguish between included logic symbols,
      brackets, data types.

      \begin{figure}[h!]
        \centering
        \includegraphics[width=0.5\textwidth]{images/site_click_to_insert.png}
        \caption{`Click to insert' buttons for easily entering logic symbols}
      \end{figure}

      Logic symbols could be input both with key shortcuts and with dedicated
      buttons. For instance, the AND symbol could be input by typing
      forwardslash then backslash. This ensured that it was easy for new users
      to start using the web app (using the provided buttons), whilst also
      allowing experienced users to quickly input larger queries (using key
      shortcuts).

      \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{images/site_schema.png}
        \caption{Database name, tables and table schema representation}
      \end{figure}

      Information about the database schema was prominently displayed at the top
      of the main page. The data included the database name, table names, table
      attributes, attribute types and marking of the primary key. Without this
      information the user could easily forget the exact names of each attribute
      that they wanted to use, or not know the correct datatype to use in their
      logic query.

      \begin{figure}[h!]
        \centering
        \begin{minipage}{0.49\textwidth}
          \centering
          \includegraphics[]{images/site_schema_insert.png}
        \end{minipage}%
        \begin{minipage}{0.49\textwidth}
          \centering
          \includegraphics[width=\textwidth]{images/site_schema_inserted.png}
        \end{minipage}
        \caption{Before and after screenshots of predicate insertion}
      \end{figure}

      To further simplify and increase the speed of writing logic queries,
      each table attribute had an arrow button placed next to it. When clicked,
      the button would insert the appropriate logic syntax into the logic query
      field. This feature helped to significantly reduce the number of syntax
      errors caused by small spelling mistakes, and helped the user to input
      correct logic syntax.

      The site was designed with the fewest number of pages possible, in order
      to maintain its simplicity and ease of use. When a user browses to the
      site, the homepage is the logic translation page. This allows the user to
      begin working straight away, with no setup costs or many pages to first
      browse through.

      In addition to the translation page, there is also a help page and a
      database setup page. The help page provides an easy to understand guide
      on how to write and form logic queries, whilst the database setup page
      allows the user to specify their own database to connect to and run
      queries on. The links to these two pages are placed at the top of the page
      so that they are easily accessible.
 
      Picked nice colours and text styles.

  \subsection{Middleware}
    \subsubsection{Celery and RabbitMQ}
      By default web.py is multi-threaded, meaning that each users request is
      handled in a separate thread, this allows multiple users to send HTTP
      requests to the server at once. 
      
      Because of this, it would indeed be possible to run the translation code 
      in the same thread that services a users web-request. However in an effort 
      to make our project scalable we decided to use a 3 tiered architecture. 
      
      A User Makes a request to the web server, the web server dispatches a job 
      to RabbitMQ, a Celery worker listens to the RabbitMQ task queue and when a 
      new job arrives, it pulls it off the task queue, processes it and posts the 
      result back to RabbitMQ. Here RabbitMQ is acting as a message broker for the
      server to communicate to the Celery worker. Currently our implementation 
      assumes that all three services are running on the same machine, however it
      is very easy to edit the configuration so that the platform can scale to an 
      arbitrary number of separate workers on different physical devices.

      In addition, each users settings are stored in a session variable,
      allowing for multiple users to use the site, to connect to multiple
      databases.

      \missingfigure{Insert diagram of architecture here.}

      To achieve this, the Celery~\cite{celery} asynchronous
      task queue and RabbitMQ~\cite{rabbitmq} message broker were
      used. Each logic sentence to translate could be easily packaged into its
      own task and added to a queue to be translated.

      Below is an example of how these services are used in our project: 
      \begin{minted}{python}
        #code.py
        import task as worker
        result = worker.addToProcessingQueue.delay(logic, web.schema)
        response = result.get()
        #worker.py
        @celery.task
        def addToProcessingQueue(logic, schema):
      \end{minted}

    \subsection{Logic to SQL translation}

      In this section, we will focus on the implementation of the translation
      between first-order predicate logic and SQL.

      The problem can be broadly thought of as the translation of a source
      langauge into a target language, a task that is commonly performed by
      compilers. Another alternative would have been to create an interpreter.
      Interpreters are very similar to compilers, in that they manage the
      translation from one language to another, but instead of translating the
      whole program into machine code, they create an intermediate
      representation, and then translate and execute the representation at
      runtime.

      We chose to create a compiler for two reasons. The first was that a
      logical query is not useful if part of it is malformed. In an interpreted
      language, an expression that is malformed but not executed is allowed to
      exist. The second was mainly due to the previous experience of the group
      members, all of whom have studied a Compilers course.

      We used the PLY library, which is an implementation of GNU's LEX and YACC
      written in Python, in order to generate a parser and lexer for our
      project.
    \subsubsection{Lexical Analysis}

      First we specify the tokens that make up the lexical stage of parsing
      using regular expressions. This can be seen in
      \texttt{lexer.py}. PLY automatically identifies something as a token if it
      has the following format:
      \begin{minted}{python}
        t\_TOKENNAME
      \end{minted}
      It can either be a constant, or for more complex tokens such as
      identifiers it can be a function.

      The lexer is generated by calling ply.lex.lex(). The lexer splits the
      source language (in this case, the logical query) into these tokens that
      can then be analysed in the parsing stage.

      \subsubsection{Building an Abstract Syntax Tree}

      The logic queries that we parse form an LALR(1) (Look-Ahead, Left to
      right, Rightmost deriviation) grammar\cite{sittingacrossthere}. The
      grammar is described in \texttt{parser.py} in a notation similar to BNF.
      PLY's implementation of YACC requires you to define your rules as a
      python docstring. PLY then parses the docstring to generate a full
      representation of the grammar, calling the defined functions as
      necessary. A parameter to the function is $p$, an array where $p[i]$
      holds the value of the i\textsuperscript{th} (1-indexed) token or state matched in the
      grammar. The function should leave the resolved state object in $p[0]$.
      As an example, the following function specifies the rule for a bracketed
      $formula$, which resolves simply to the $formula$ state. 
      \begin{minted}{python}
        def p_formula_bracketed(p):
          'formula : LBRACKET formula RBRACKET'
          p[0] = p[2]
      \end{minted}

      As the grammar is reduced, an abstract syntax tree is built from the
      bottom up. The AST is made up of \texttt{Node} subclasses found within
      the \texttt{ast/}
      directory. Each subclass will contain its own internal representation of
      the language construct, but most follow the general convention of having
      a number of children nodes.

      For example, take the simple logical query below:
      \begin{multline}
        \exists x(films.title(x, y))
      \end{multline}

      This will build the following abstract syntax tree.

      \todo{Create simple AST graphic}

      Below you can find a more complex reduction rule, a more complicated
      logical query and its resulting AST:

      \begin{minted}{python}
        ### A /\ B ###
        def p_formula_and(p):
          'formula : formula AND formula'
          lineNo = p.lexer.lineno
          pos = getPosition(p.lexer)
          p[0] = ast.AndNode(lineNo, pos, p[1], p[3])
      \end{minted}

      \begin{multline}
        \exists x(films.title(x, y) \land films.director(x, z))
      \end{multline}

      \todo{Create more complex AST graphic}

      At the end of the parsing stage, the root
      node will be accessible to then perform symbol table, and later code,
      generation.

    \subsubsection{Generating a Symbol Table}

      Now that a well formed AST has been constructed, the tree is then
      traversed, populating a symbol table. Here we take the meaning of a
      symbol table to be similar to that of a compiled object file, a structure
      that holds information needed to locate and relocate a program's symbolic
      definitions and references.\cite{symtab}

      Below are justifications for the use of a symbol table.

      \paragraph{Scoping}
        In a logic query, is is possible to reuse variable names in an inner
        scope that exist in an outer scope. For example, take the following
        query in logic.
        \begin{multline}
        \exists x(films.title(x, y) \land \forall x(films.title(x, y) \land
        films.length(x, z) \land z > '1:00:00)) \label{eqn:scoping}
        \end{multline}
        This query returns the set of all singleton tuples $\{films.title\}$ such
        that for all films with the same title, they all are longer than one
        hour. Note that we quantify $x$ twice, once with an existential
        quantifier and once with a universal quantifier. For this reason, the
        inner subquery's definition of x is different to the outer subquery's.
        For this reason, we must be able to distinguish between different
        scopes. This can be achieved by using a hierarchical symbol table.

      \paragraph{Unification}
        Similarly, if two variables are in the same scope, it is important that
        they are unified; both variables should refer to the same underlying
        value. Referring back to equation~\ref{eqn:scoping}, even
        though $y$ exists within both scopes, because it is not redefined
        within the inner scope, each instance of the variable refers to the 
        same value.

      \paragraph{Free and Bound Variables}
        We typically rely on a variable being defined by a quantifier, but
        there are many occasions where this is not the case. Referring back to
        equation~\ref{eqn:scoping} again, we see that $y$ is returned as it a
        free variable, a convention that was agreed on back in
        section~\ref{sec:freebound}. As $y$ has not been defined before, it is
        necessary to add the declaration of $y$ into the symbol table, in a top
        level 'global scope'. This way, any redeclarations of $y$ are caught
        lower in the table, but any instance of $y$ that exists outside of a
        scope in which it has been redeclared will have a reference to this
        global instance.

      The symbol table structure is defined in the file
      \texttt{codegen/symbtable.py}. A symbol table contains a dictionary of
      values, mapping identifiers to Node subclasses. It also contains an
      optional pointer to a parent symbol table. The symbol table class defines
      a $lookup$ method, which searches for a Node subclass for a given key
      within its dictionar. The method will propogate to a higher scope if the
      definition is not found, otherwise it will return the definition.

      Each node has a reference to the symbol table that exists within its
      scope. This way, a node can resolve a variable instance to it's
      declaration, solving the issues of unification, free and bound variables
      and scoping.

    \subsubsection{Generating an IR}

    \subsubsection{Generating SQL}
      Lots of scope to talk about each different construct (SELECT, SELECT FROM, 
      JOIN, NATURAL JOIN etc.) and any optimisations we did.

    \subsection{Back End}

     \todo[inline]{
       Backend\\
       -------\\
       Config Parser - parsing configuration files\\
     }

    \subsubsection{Interfacing with the database}

      We used several publicly available Python libraries in order to make 
      it very easy to query SQL databases. In particular we prioritised
      producing modular code so that our team members who were working on the
      other parts of the project could concentrate on their task, without having
      to worry about how the backend querying worked.

      In order to run an SQL query you have to run through the following steps:

      1. Grab some configuration (username password etc)
      2. Open a connection
      3. Run a query
      4. Close the connection.

      Just using psycopg2, the PostgreSQL adapter for Python, it would take
      several function calls to achieve each step. For example in order to run a
      query and return the results just using psycopg2 (assuming you already
      have a connection object setup) you would need to do the following:

      \begin{minted}{python}
      def query(con, query):
        result = {}
        try:
          cur = con.cursor()
          cur.execute(query)

          # Convert all rows to string representation
          result['rows'] = []
          for row in cur.fetchall():
            result_row = []
            for val in row:
              result_row.append(str(val))
            result['rows'].append(result_row)

          result['columns'] = [desc[0] for desc in cur.description]
          result['status'] = 'ok'
        except psycopg2.DatabaseError, e:
          result['error'] = str(e)
          result['status'] = 'db_error'
        finally:
          return result
      \end{minted}

      we decided to come up with the following architecture: 
      1. Creation of a configuration data structure. (either from a file or user
         entry)
      2. using that configuration data structure to create a connection object
      3. query the database by passing a function a connection object and a
      string containing the SQL query.
      4. close the connection object when all queries are finished

      This way, details about the database configuration and connection 
      can be passed around in an opaque manner. The final code for all four
      steps now looks like the following: 

      \begin{minted}{python}
        configData = cp.parse_file('dbbackend/db.cfg')
        con = pg.connect(configData)
        queryResult = pg.query(con, sql)
        con.close()
      \end{minted}

    \subsubsection{Codifying Database Schema}
      In an effort to make our project extensible, the web platform that performs 
      the logic to SQL translation uses an XML representation of the database
      schema. In this way the translation engine is platform agnostic with
      regards to which particular SQL implementation the web server is running.
      Indeed as long as an XML file is provided it would be possible extend the
      system with minimal effort to run the translation engine in an 'offline' mode 
      where it is not connected to a database and it does not run the translated 
      query, instead it simply presents the query to the user.

      This XML representation is automatically generated on program startup.
      Currently the project only supports PostgreSQL implementations, however it
      has been designed so that support for other SQL implementations can be
      added modularly. In order to extend support to other platforms all that is
      required is to add a \%SQL\%\_backend.py which provides the functions
      described above, then switch out the import in generate\_schema.py to use
      the desired implementation.

      \todo[inline]{Talk about how the schema is stored (Python dict), accessed
        via the Schema class, and how it can be returned as JSON for the UI via
      a GET request to /schema}

      (\todo{LOL} Max did you ever fix your multiple database issues? Also I
      think we're stuck with PostgreSQL databases only?)

    \subsubsection{Semantic Analysis}
      One of the most important error checking methods we employ is semantic
      analysis - this is looking at the SQL query, combining its syntax with the
      meaning (semantics) derived from our database schema, and identifying
      inconsistencies and errors. In this initial version of our logic to SQL
      translator, we check the predicates of our SQL query against the known
      relations and attributes present in our schema. If a relation, attribute,
      or relation attribute pair doesn't exist, an exception is raised which is
      caught and an error message is displayed to the user. This error would
      allow the user to quickly identify any typos or invalid predicates.

      \begin{figure}[h!]
        \centering
        \includegraphics[width=1.0\textwidth]{images/error.png}
        \caption{A user being informed of a semantic error: the relation
          'cornucopia' does not exist in the schema.}
      \end{figure}

    \subsubsection{Error Handling}
      There were many stages of the translation process which could have
      encountered errors that needed to be sent back to the user. To deal with
      this, an exception-based error reporting system was created.

      The basic idea behind this was to put each stage of the translation in a
      \texttt{try: catch:} block. Then, if a stage encountered a problem it
      would raise exceptions containing the error information, which would cause
      the relevant \texttt{catch:} block to be called. This block would then add
      the exception as a Python dictionary to the AJAX response, and when sent
      back to the UI as JSON it would be able to construct a useful error
      message based on the exception data.

      Therefore, a Python exception class was created for each type of error
      that could be produced, containing all the relevant information about the
      error. Additionally, a special exception group class had to be created to
      allow the raising of multiple exceptions by any stage of the translation
      process (Python can only raise a single exception at a time).

      In the UI, if the `status' string of the AJAX response is not ``ok'', this
      indicates an error response. In this case the value is switched upon based
      on various error status codes, for example `parse\_error', and the `error'
      member of the response is then interpreted accordingly. This involves
      displaying an error line in the `console' section of the HTML page for
      each exception returned by the back end.

  \subsection{Testing}
  To ensure the reliability of our software we needed a testing framework that
  we could run easily. We chose to write standard Python unittest tests and on
  top of these used the nose testing framework to call and profile the runs.
  Using nose gave us a clear indication of how many of our tests have passed
  and gave us an idea of how our system was performing.

  We also used paste, which is a set of libraries that allow you to autoate
  tests which interact with a web page. This gave us the ability to test
  sending requests from our interface, simulating user interactions in a
  repeatable and fast manner.


\section{Evaluation}
  \todo[inline]{Evaluate your deliverables e.g. performance, usability, etc.
  Summarise testing procedures + relevant testing results}

  \subsection{Evaluation Using Data}
    \subsubsection{User Interface}
    Ease of use was a key requirement of our project. As such we collected
    metrics of user data in order to quantify each user's interaction with the
    site. To measure the ease of inputting queries and to gain a picture of the
    learning curve of the site, we recorded the average length of inputted
    queries over time per user, and whether these queries were valid (shown
    by whether they were able to be parsed).

    \begin{minted}{python}
      # Profiling code to record query length versus time taken
      # for the user to enter it
      statistics = {'query_length': len(logic_to_translate),
                    'error': error,
                    'time to enter query': form.time.get_value()}

    \end{minted}
    
    \subsubsection{Application Performance Analysis}
    Because the response time per query would be a large factor in a user's
    experience of the site, we were very interested in recording the performance
    of each component of our site. Thus we were able to quantify the final
    performance of our product at the end of the project, by calculating the
    average time to process each query.

    The python testing framework Nose was used with the '--with-profile' flag.
    This allowed us to see how long functions and pieces of code were taking to
    execute per query. Towards the end of the project, we had reduced each query
    to run in less than 1.5 seconds, which we believed was an acceptable time
    for a user to wait, and was also in line with other online services.

    Aside from empirical data analysis, another important factor of application
    performance was the perceived performance by the user. Talking to users and
    gathering survey data using the online SurveyMonkey platform left the team
    with a more meaningful representation of the relative speed of the
    application with respect to the users expectation, with 71.4% of users
    answering that the were "happy with speed of query processing", and 28.6%
    happy.

    \subsubsection{Verification of Logic to SQL Translation}
    By constantly adding new tests based on queries that were either produced
    within our group, or suggested by our supervisor and others, we quickly
    built up a large set of test cases. These test cases provided the assurance
    that our application was able to successfully handle all syntactically valid
    queries that a user could enter. The expected results of each test case,
    the SQL queries that were to be produced, were built up through conversations
    with users about their views of the semantic meaning of each logic query
    they tried. By the end of week 7, we had over  60 test cases which all
    successfully passed.

  \subsection{Use of Data for Making Decisions and Reaching Conclusions}
    \subsubsection{Site Usability Improvements}
    Another insight we gained from our survey was that users wanted a tutorial
    or information page on the site to help them learn how to use the
    application. This was clear to see as over 70% of users wanted such a
    system. In response we produced an information dialogue box which explained
    the key features and elements of the application.

    Survey results and information page implementation
 
    \subsubsection{Targeting Performance Increases}
    By using the Python nosetests profiling throughout the project, we were
    able to target improvements at the pieces of code which had the highest
    performance cost; maximising improvements per time spent writing code.

    After profiling our code we found that queries to our database took a large
    percentage of time to run compared to the rest of the code (1.822s/2.307s =
    79%) we worked to reduce its execution time, as this would yield the largest
    performance benefit for time spent working. We modified our query() function
    so that it only read the connection parameters for the database once,
    whereas it had previously been reloading the parameters each time query()
    was called.

    Application component runtime profiling

  - talk about how our plan changed, how things went wrong

\section{Conclusion and Future Extensions}
\todo[inline]{What did you learn? What might you have done differently?
  How would you build on what you have done?}

  All the project management tools barely worked. We didn't have a week where
  TeamCity didn't break. Also don't forget to delete pyc files! And get
  pythonpath to work!

  We used git rather badly - nobody ever pushed to master because it never
  worked. Basically codegen was the sole useful branch (apart from ui maybe).

  \subsection{Future Extensions}
    Better user support (login system, multiple servers to do translation,
    more databases supported, not just PostgreSQL).

    \subsubsection{Other logic things to support}
    logic functions previously defined, so don't have to keep writing the same
    thing out loads of times?

    \subsubsection{Further Semantic Analysis}
    \todo[inline]{Can anyone think of anything beyond type checking?}

    \subsubsection{Support more database backends} 

\section{Project Management}
  \todo[inline]{Planning, group organisation, breakdown + task allocation etc.}

  \subsection{Planning}
    \todo[inline]{TODO Should we discuss here how everything went wrong, and
    revisions were missed entirely?}

    As we have learnt from previous projects, inside and outside the
    department, it is critical to thoroughly plan a software engineering
    exercise of this scale before beginning to implement features and write
    code. As discussed in our introduction section, we decomposed our problem
    of translating first order predicate logic to SQL into several key
    subgoals. This enabled us to outline a core feature set which we wished to
    implement and gave us a much clearer idea of how to begin tackling the
    problem.

    Moreover, we initially split the project up in to five major 'revisions'.
    Each revision added a certain amount of functionality to our project, and
    allowed us to continually build upon features already implemented. This
    incremental method of development is one of the key ideas of the Agile
    development methodology to which we adhered to throughout the project.
    \todo[inline]{TODO citation} Additionally, with each revision having a
    deadline, it gave the team a clear plan of what was to be implemented at
    what time.

    \begin{table}[H]
      \centering
      \begin{tabular}{| l | p{0.6\textwidth} | l |}
        \hline
        \textbf{Revision} & \textbf{Necessary Steps for Completion}
          & \textbf{Completion Estimate} \\
        % \multicolumn{1}{p{0.6\textwidth} |}{\textbf{}}
        \hline
        1 &
          \begin{compactitem}
            \item Set up basic web server;
            \item Set up and create a database;
            \item Create simple UI \& communicate with server;
            \item Create 5 sample Logic to SQL translations;
            \item Set up development environment.
          \end{compactitem}
          & 17/10/13 \\
        \hline
        2 &
          \begin{compactitem}
            \item UI sends predicate logic to web server;
            \item Web server parses the logic into an AST;
            \item Backend translates the AST to SQL for basic \texttt{SELECT
              FROM} queries (i.e. projection only);
            \item Create 5 more sample Logic to SQL translations.
          \end{compactitem}
          & 25/10/13 \\
        \hline
        3 &
          \begin{compactitem}
            \item Expand backend parser grammar to include more advanced use of
              SQL queries (selection \& projection);
            \item Dynamic table selection in logic - "smart" logic predicates,
              e.g. \texttt{updated(x)} vs. \texttt{customer.updated(x)};
            \item Hook into database.
          \end{compactitem}
          & 1/11/13 \\
        \hline
        4 &
          \begin{compactitem}
            \item User-defined functions;
            \item Configurable UI - database settings, and possibly changing the
              UI depending on the user's ability;
            \item Extend backend to use SQL joins.
          \end{compactitem}
          & 8/11/13 \\
        \hline
        5 &
          \begin{compactitem}
            \item Contingency time built in for bug fixes, or additional
              features/extensions depending on the project status (e.g. Semantic
              checks of logical statements before translating to SQL).
          \end{compactitem}
          & 15/11/13 \\
        \hline
      \end{tabular}
      \caption{Feature sets for each revision, as defined at the start of the
        project}
    \end{table}

  \subsection{Group Organisation}
    At the beginning of the project, as discussed previously, we outlined the
    goals and required features of our project, and split them up in to three
    rough areas - front end (handling user interactions),
    middle end (communicating between client and server), and
    then the back end (translation and database access).
    We initially split our group of six in to three teams,
    corresponding to each of the subsections of our project. Members were
    allocated to each team depending on their strengths - initially Ted and Tom
    were to complete front end work, Sam and Mitchell to do middle end, and
    Sean and Max on back end. These were just initial roles, and we found that
    throughout the project, roles could be temporarily switched around to put
    extra work in to tasks that took longer than expected.

    One of the key ways we kept our group organised was through weekly
    meetings.  During each weekly meeting, we discussed what tasks were to be
    completed that week, and spoke in detail about any concerns members had
    about the tasks that had been completed previously. We would also analyse
    the previous weeks performance, and make changes to our plan accordingly.
    This falls in line with a key principle of Agile development, where a team
    must be adapatable to change. An example of such a change is when we
    realised implementing the parser would take significantly longer than
    originally thought. This lead us to modify our feature set, reallocate
    group roles temporarily, and modify deadlines and revision subgoals.

  \subsection{Tracking}
    Once an overarching plan was agreed on, we split each major goal of each
    revision in to smaller tasks. For each of these tasks, we then allocated a
    numerical estimate of the time it would take to complete, on a scale of 1
    to 5, where 1 signified a short task and 5 signified a very time consuming
    task. The estimate of time a task would take was initially based on
    previous experience on other software engineering projects the group
    participated in, but as the project progressed, we applied knowledge of how
    prior tasks went to more accurately assign estimates.

    A typical practice in Agile development is for developers to write down
    their tasks on cards, and then place them on a physical board (known as an
    "information radiator") located in sight of all developers on the project.
    Upon this board are "swim lanes"; columns which cards belong to, and denote
    their progress (e.g. "to do", "doing" and "done"). Each task has a time
    estimate (as discussed above), and is assigned to one or more people to
    complete.  This allows every developer to know exactly what is happening at
    what time, what has to be done, and what has been completed. For our group,
    working in DoC labs and at home, this would obviously not be practical.
    Therefore, we used web-based project management software called
    Trello. This software behaves in the same way as a physical board, but is
    accessed via a web based interface which can be edited by all members of
    the project.  Additionally, each card has a hashtag which denotes which
    revision a task belongs to, or if a stricter deadline is required, a date
    for completion can be manually added.  Virtual cards are moved between each
    swim lane as the task progresses. The group found this tool indispensible
    for communication and keeping on track with the project.

    \todo[inline]{TODO Citations for the above agile stuff}

    \begin{figure}[h!]
      \centering
      \includegraphics[width=0.9\textwidth]{images/trello.png}
      \caption{A screenshot of our Trello board toward the end of our project.}
    \end{figure}

    \todo[inline]{The remainder of this section is copypasta from a Chatley report. Sean will go through this tomorrow. DO NOT DELETE ANYTHING IN THIS SECTION ABOVE THIS MESSAGE!!!}
 \subsection{Team Metrics and Analysis}
Throughout the project, the team actively gathered data regarding the performance of the group. This data was analysed and helped the group to track and measure group activity week on week, enabling early detection of any pitfalls or areas where the group may be struggling. Quantitative data was key for analysing group performance as it allowed us to quickly get a sense of the status of the team. This data was brought up in our weekly meetings, and was used to ensure we stayed on track and allowing us to set and modify goals for the week ahead.
\subsubsection{ Trello and Team Velocity}
Our group made extensive use of Trello, which mostly provides qualitative data; however, we took some of this data in isolation and converted it into a quantitative form. 
Each Trello card is marked with a revision number, contained within a hashtag.
We can filter by this hashtag (e.g. \#rev1) and quickly see which tasks are marked as "to do", "in progress" and "done" for a given revision. In addition to looking at this during the implementation of a revision to get an idea on progress, we can look at it at the end of an iteration to see how much work was completed and how well we met our goals. Combining this with the difficulty of each completed task, we can derive the velocity of our work - a quantitative measurement of our work speed. 
Week on week, we also analysed our average velocity and planned future iterations, taking in to account the amount of work completed in previous iterations. As part of quantitative analysis, we plotted this average velocity on a graph. On the whole, our velocity remained constant throughout the majority of the project, with the notable exception of small bumps at the start. The conclusions we drew from this are summarised in '3.1 - Decision Making', in which we recall using two methods of measuring progress (velocity and burndown) to influence the performance of the team.
\subsubsection{ Lines of Code}
One metric that some development teams use to evaluate their productivity and state of their project is to count the number of lines of code written in a given agile sprint or time frame. Each group member can also be analysed on their individual code contribution. Such statistics can be gathered from our version control logs and plotted on a simple graph.
However, we do not believe that analysing lines of code is an accurate metric of evaluating the status of the project - good code should be brief, and such a metric encourages "copy-paste syndrome". Moreover, certain aspects of our project (e.g. the parser) are complex and whilst the code for this may not be as long as other sections, it requires a lot more thinking per line. Library imports also have the potential to inflate lines of code added, which can distort these figures.
\subsubsection{ Test Coverage and Successful Builds}
We found far more useful development metric to be looking at the number of test cases we pass in each iteration. As discussed in previous reports, we make use of an expansive cornucopia of tests and employ the test driven development (TDD) methodology (in addition to Agile). These test cases act as an informal specification, and along with our Trello cards, show us how many features we have implemented (passed tests), and how many we have yet to implement (failed tests). Through the use of TeamCity, our continuous integration server, feedback is given frequently on a push to the testing branch on Git, so we can continually evaluate our product. The group looked at which tests were failing to inform what features or problems should be addressed in future agile sprints.
\subsubsection{ Other Methods}
Two other methods we considered are lead time and cycle time. Lead time is an evaluative measurement borrowed from manufacturing - it's the amount of time between the identification of a requirement and its implementation. Cycle time is similar, except rather than being the time until the user or customer sees the feature implemented, it's the time until the developer sees it implemented. Our group decided that this was not an informative evaluative technique - not all problems are equal, and some will naturally take longer than others. We therefore preferred to measure task completion via velocity, as discussed previously.
\subsubsection{ Continuous Evaluation}
When we evaluated our progress on the whole we saw that the burndown graph and velocity metrics recorded were consistent. However, the issue was raised that whilst front end development was going as planned we had a significant lack of progress with the backend parser and SQL generator.
We concluded that by joining all the tasks together the good progress in the front-end was masking the lack of progress with the backend development, this was also partly due to us underestimating the time it would take us to build an extensible infrastructure. The same issue also plagues the velocity graphs, we maintained a reasonably constant velocity throughout the project but found the ratio between the front and back end development varying.
*
*
A burndown graph for the whole project. The top line represents the total cumulative number of hours remaining, with the middle and bottom lines representing the number of hours remaining for the subtasks of back-end and front-end development.
A 'velocity' graph, displaying the total number of hours of work completed by the team on a weekly basis. The graph hits a peak on the week beginning 28/10/13, where the team cut scope, thus artificially increasing the amount of work 'completed'. The dip following this coincides with group members other commitments (coursework deadlines from other courses).
With future iterations, we decided to give a more reliable estimate (or even over-estimate) to tasks, and to break bigger tasks into smaller chunks. After this, we noticed that the data collected was much more representative of our actual progress. We extrapolated our burndown graph for each subsection, and found that those working on front-end development were scheduled to finish prior to the final week, even before contingency time, whereas backend development was not scheduled to finish until February 2014! We drew two conclusions from this information; we needed to move one person from front-end to back-end, and we needed to reduce the subset of logical queries that we planned on translating and instead focus on ease of use, reliability and the quality of error messages returned to the user. 
The ability to alter the specification of a project dynamically is a testament to the agile methodology that we adopted, which values, '... changing requirements, even late in development.' 
  \subsection{Version Control and Automated Testing}
  -- "You shall not pass" commit msg. Definitely needs the picture.

    -- Trello, Git integration

    -- Burndown chart

    -- nosetests

    -- CI server
 

\section{Conclusion and Future Extensions}
\todo[inline]{What did you learn? What might you have done differently?
  How would you build on what you have done?}


  At the start of this project no member of the group had any significant
  experience programming in Python, despite this we found it very easy to pick
  up and learn on the fly. However because of this, some of the early code we
  wrote had to be refactored several times after learning significant features
  of the language which allowed us to simplify it considerably, we found the use
  of Python dictionaries to be particularly useful in many area's of the
  codebase allowing for a well defined error handling system to be rapidly built
  up.

  Upon reflection, at the beginning of the project we started well, planning
  in advance and making sure team members were kept up to date with the
  technical details of how the project worked. However as time progressed this
  changed, there was a tendency for some members of the project to work on an
  area without informing others of their intention, or specifying abstractly
  how they were solving certain problems. For example at the beginning of the
  project everybody had a good understanding of how we would parse the grammar
  for predicate logic and how we could build up a syntax tree, but later on
  during the codegen phase a few members (whilst doing sterling work) 'ran away'
  with the project resulting in a large chunk of code which only a few people
  understood; this greatly reduced how many people could contribute to that part
  of the project without spending a significant amount of time figuring out what
  was being produced, which in turn slowed down development of certain features.
  Really what we have learnt from this is that documentation is key, indeed if the
  group were to do the project over again, we would have made sure that for each
  agile sprint a group member would be tasked solely with the role of 'Technical
  Writer' who's job it would be to produce a set of specifications for each part
  of the project worked on that week. 


  \subsection{Future Extensions}
    During the course of this project we did not get time to implement several
    features we had initially hoped to provide. 

    Better user support (login system, multiple servers to do translation,
    more databases supported, not just PostgreSQL).

    \subsubsection{User Defined Predicates}
    Initially we wanted to have a mechanism by which as user would be able to
    define the meaning of a predicate which they could then use in their query.
    For example being able to define a predicate
    \texttt{stars\_in(aname,ftitle)} which
    relates an actor with a name 'aname' staring in a film with title 'ftitle'

    \begin{verbatim}
      stars_in(aname, title) |= films_title(f,title) /\ casting_fid(c,f) /\
      casting_aid(c,a) /\ actors_name(a,aname)
    \end{verbatim}

    \subsubsection{DML and More DDL Operations}
    Currently we have no facility to express operations like INSERT, DROP, 
    CREATE, UPDATE, and DELETE, we only operate on the 'read-only' part of DML
    there is a vast set of SQL which we are unable to provide a mapping for
    in our system.
    \subsubsection{Further Semantic Analysis}
    \todo[inline]{Can anyone think of anything beyond type checking?}
    
    \subsubsection{Security enhancements}
    Currently our project is vulnerable to several SQL injections. While working
    on the project we prioritised the initial task over the security of the
    system. Additionally, whilst the connection to our site is secured via
    HTTPS, passwords to the database are still sent in 'plaintext' over HTTPS.
    \subsubsection{Being able to download and save results}
    Currently the only way the result of a query, which has been translated and
    run, is displayed is via the web page. If a user wanted the information in a
    different format it would be nice to provide some sort of mechanism to
    download that data.
    \subsubsection{Support for Additional Database Implementations} 
    Currently our project only supports Postgres databases we would also have
    liked to be able to support several other major implementations like MySQL and
    Microsoft SQL Server, however due to lack of time this feature was removed
    in favour of focusing on making sure that the logical translations produced
    were correct. 
    \subsubsection{UI Improvements}
    There are several niceities which would be useful to have, increasing
    responsiveness of the system whilst running long queries (providing the user
    feedback to let them know that it hasn't crashed), saving a recent history
    of queries, developing a tablet version of the site, being able to truncate
    the ouput of queries if they are too long, caching results of recent queries
    ot improve performace. 


   
    

\section{Bibliography}
  \printbibliography

  --TODO Agree on referencing practice. The library recommends Harvard. --

  https://workspace.imperial.ac.uk/library/Public/Harvard\_referencing.pdf

\appendix
\section{Appendix}
  The appendix is optional, and does not count towards the 35 pages. It may
  contain thing like: User guide, installation instructions; more extensive
  design, testing, statistics etc.

  \subsection{Installation Guide}
    For installation of our logic to SQL translator, we strongly recommend
    starting with a vanilla installation of Ubuntu 12.04 LTS, for which a guide
    follows. If you wish to install on another operating system (it has been 
    confirmed to work on Ubuntu 13.10 and Mac OS X Mavericks),
    limited help is given.

    \begin{enumerate}
      \item Firstly, install dependencies via \texttt{apt-get}, or similar
        package manager for your system (e.g. \texttt{brew} for Mac):
        \begin{verbatim}
          sudo apt-get install python-dev python-pip python-pastescript
          python-nose postgresql libpq-dev libxml2-dev libxslt-dev
          rabbitmq-server openssl
        \end{verbatim}

      \item Secondly, install the required Python packages via \texttt{pip}.
        This should work regardless of platform (replace \texttt{sudo} with the
        command to elevate to superuser privileges for your platform):
        \begin{verbatim}
          sudo pip install psycopg2 celery lxml
        \end{verbatim}

      \item Extract the Logic to SQL project tarball to your desired
        installation folder.

        Now, modify \texttt{INSTALL\_DIR/dbbackend/db.cfg} for your PostgreSQL
        database. The format of this file is intuitive.

      \item The project requires an SSL certificate. The certificate
        files need to be in \texttt{INSTALL\_DIR/certs} with files
        \texttt{server.key, server.csr} and \texttt{server.crt}.

        To generate your own certificates, see:\\
        \url{http://chschneider.eu/linux/server/openssl.shtml} or the
        documentation at \url{http://www.openssl.org}

      \item \textbf{Optional}: If you decide to use
        the default database settings (the \texttt{filmdb} database), then you
        can test your installation by running our test suite. This runs
        our test suite to confirm your installation is configured correctly.
        Run the following in \texttt{INSTALL\_DIR}.

        First, type \texttt{python dbbackend/generate\_schema.py} to generate
        the schema for the test cases to use. Now. run \texttt{nosetests}. If
        all tests pass, your installation is configures correctly. Note that
        our test suite is only compatible with the \texttt{filmdb} database.

      \item The server can now be started by running the following:

        \begin{verbatim}
          celery --app=task worker &>> celery.log &
          ./run PORT_NUM &>> webpy.log &
        \end{verbatim}

        The software will now be accessible via
        \texttt{https://localhost:PORT\_NUM}. Logs will be available in
        \texttt{celery.log} and \texttt{webpy.log}. Note that if you wish to
        run the server on port 80, you must execute the \texttt{run} script
        with superuser privileges (i.e. with \texttt{sudo}).

        As the project is designed to run as a daemon, these commands can be
        added to a boot-up script. The processes may be terminated in the
        normal way.
    \end{enumerate}

  \subsubsection{Other Operating Systems and Dependencies}

  To install our project on other operating systems, the following dependencies
  must be met (including version tested with).

  \begin{itemize}
    \item Python, including dev tools (2.7.x) - \url{http://python.org}
    \item Pip (1.0) - \url{http://www.pip-installer.org/en/latest/}
    \item Nose (1.1.2) - \url{https://github.com/nose-devs/nose/}
    \item Paste (1.7.5.1) - \url{http://pythonpaste.org/}
    \item PostgreSQL (9.1) - \url{http://www.postgresql.org/}
    \item libpq-dev - \url{http://www.postgresql.org/}
    \item libxml2-dev - \url{http://www.xmlsoft.org/}
    \item libxslt-dev - \url{http://xmlsoft.org/XSLT/}
    \item RabbitMQ - \url{http://www.rabbitmq.com/}
    \item OpenSSL - \url{http://www.openssl.org/}
  \end{itemize}

  The installation then broadly follows step 2 and onwards from the above
  guide. 

  \textbf{Note:} We have determined an incompatability between our project and
  the Gentoo operating system. We make use of the RabbitMQ message server,
  which has a dependency on the Erlang language libraries. These libraries
  are notoriously difficult to install on Gentoo, as confirmed by a visit to
  the Erlang IRC channel.

  \subsection{Schemas}

    \subsubsection{Films Database - filmdb}

      \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/site_schema.png}
        \caption{Basic 'filmdb' schema, containing information about films,
        their actors, directors etc.}
      \end{figure}

      The films database is based upon the example database used in teaching
      the first year databases course.

    \subsubsection{Shakespeare Database - shakespeare}

      \begin{figure}[H]
        \includegraphics[width=\textwidth]{images/site_schema_shakespeare.png}
        \caption{More advanced 'shakespeare' schema, containing information
        about his literary works.}
      \end{figure}

      The Shakespeare database is taken from \\
      \url{https://github.com/catherinedevlin/opensourceshakespeare}, which
      itself is based on the Open Shakespeare Project, \\
      \url{http://www.opensourceshakespeare.org/}.

  \subsection{Example Queries}

  The following queries make use of the \texttt{filmdb} and
  \texttt{shakespeare} schemas, outlined above.

    \subsubsection*{"Get me all films, their titles, directors, and when they
      were made"}

      Logic:
      \begin{gather}
        \exists x(films.title(x, y) \land films.director(x, z) \land films.made(x, a))
      \end{gather}

      SQL Generated:
      \begin{minted}{sql}
        SELECT films.title, films.director, films1.made
        FROM films JOIN films AS films1 ON (films.fid = films1.fid) AND
        (films.fid = films1.fid)
      \end{minted}

      This demonstrates a redundancy in our code generator whereby a
      \texttt{JOIN} is performed unnecessarily. However, the query is still
      correct and produces the correct results.

    \subsubsection*{"Get me all actors and the roles they've played."}

      Logic:
      \begin{gather}
        \exists x,a(casting.part(x, y) \land casting.aid(x, a) \land actors.name(a, b))
      \end{gather}

      SQL Generated:
      \begin{minted}{sql}
        SELECT casting.part, actors.name
        FROM casting JOIN actors ON casting.aid = actors.aid
        WHERE (actors.aid = casting.aid) AND (actors.aid = casting.aid)
      \end{minted}

    \subsubsection*{"Get me directors where all their films are greater than 100
      minutes long."}

      Logic:
      \begin{multline}
        \exists x(films.director(x, dir) \land \forall y(films.director(y, dir)
        \\
        \to (films.length(y, len) \land len > '100')))
      \end{multline}

      SQL Generated:
      \begin{minted}{sql}
        SELECT films1.director
        FROM films AS films1
        WHERE NOT (EXISTS (SELECT films2.fid, films2.director, films2.length
        FROM films AS films2
        EXCEPT
        SELECT films2.fid, films2.director, films2.length
        FROM films AS films2
        WHERE ((films2.length > '100') AND (films2.director = films1.director))
        AND (films2.director = films1.director) ) )
      \end{minted}

      \subsubsection*{"Get me character names and descriptions for all
        characters in either 'Othello' or 'Macbeth', where the characters
      speaks over 50 lines."}

      Logic:
      \begin{multline}
        \exists cid,wid(character.charname(cid, name) \land character.description(cid,
        desc) \\
        \land character.speechcount(cid, sc) \land sc > 50 
        \land work.title(wid, play) 
        \\ \land character\_work(cid, wid) \land (play = 'Othello' 
        \lor play = 'Macbeth'))
      \end{multline}

      SQL Generated:
      \begin{minted}{sql}
        SELECT character.charname, character.description,
        character1.speechcount, work.title
        FROM character JOIN character AS character1 ON (character.charid =
        character1.charid) AND (character.charid = character1.charid) CROSS
        JOIN work JOIN character_work ON (((character.charid =
        character_work.charid) AND (character.charid = character_work.charid))
        AND (character1.charid = character_work.charid)) AND (work.workid =
        character_work.workid)
        WHERE ((character1.speechcount > 50) AND (NOT ((NOT (work.title =
        'Othello' )) AND (NOT (work.title = 'Macbeth' )) ))) AND
        ((character_work.charid = character.charid) AND (character_work.workid
        = work.workid))
      \end{minted}

      This query is easier to express in our logical syntax than it is in SQL.
      However, this query does show how SQL optimisations could be applied
      before returning the query to the user, although in practice, the SQL
      server would do such optimisations before execution.

  \subsection{UML Diagram}

  - Look online to see if this can be generated by any tools.

  - Or make a reduced one, just showing main components of the project e.g.
  parser, sql generator, semantic analyser, etc

  \subsection{Design Diagrams}

  - things like sean's bullshit rough drawings from Chatley reports

  \subsection{Statistics}

  - This is just the performance metric, profiling etc diagrams from 
  Chatley coursework 4.

%\section{Anandha waffle from his site - delete once finished report}
%  Final Report ? due: 13th Jan 2014, at 16:00 (both Electronic and Hardcopy)
%
%  Contents for Final Report: The project report should not be longer than 35
%  pages (recommended length is around 30 pages), and might be organised
%  according to the following structure: see above sections
%
%  Make sure that the final report presents a coherent story. Ask advice from
%  your supervisor. You might also draw inspiration from the instructions about
%  writing up your individual project.
%
%  Bear in mind, that most of the project assessors will not have followed the
%  project throughout and will only have a short time to listen to a
%  presentation or see a demonstration. For this reason they will rely heavily
%  on the report to judge the project.
%
%  The report should be submitted to SGO in form of a hard copy, as well as
%  electronically through CATE. i.e. report.pdf. 
%
%  ------------------------
%
%  Assessment
%  Will be updated soon.
%
%  Group project
%  The group project assessment is undertaken by each group's supervisor, and
%  moderated by a larger group of assessors, who will attend your presentation,
%  and/or read your final report. The assessment is based on:
%
%      Executive Summary, 5
%      Presentation, 10
%      Group Collaboration and Management, 20
%      Report, 30
%      Technical Achievement, 35
%
%  The group project (along with the Software Engineering course) is worth 440
%  Marks for both the MEng and BEng students.
%  Overall assessment
%  The overall assessment is the sum of the group project component and the
%  Software Engineering Course in an 80:20 split.
%
\end{document}
