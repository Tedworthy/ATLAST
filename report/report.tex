\documentclass[a4paper, 11pt]{article}

\usepackage{geometry}
\usepackage{layout}
\geometry{includeheadfoot, margin=3.18cm}
\setlength{\parskip}{0.3cm} \setlength{\parindent}{0cm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array,booktabs}
\usepackage[style=numeric,sorting=debug,backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{todonotes}
\usepackage{paralist}
\usepackage{listings}
\usepackage{lmodern}
\usepackage{minted}
\usepackage[fleqn]{amsmath}
\usemintedstyle{autumn}
\lstset{language=SQL}
\addbibresource{report.bib}
\DeclareDatamodelEntrytypes{standard}
\DeclareDatamodelEntryfields[standard]{type,number}
\DeclareBibliographyDriver{standard}{%
  \usebibmacro{bibindex}%
  \usebibmacro{begentry}%
  \usebibmacro{author}%
  \setunit{\labelnamepunct}\newblock
  \usebibmacro{title}%
  \newunit\newblock
  \printfield{number}%
  \setunit{\addspace}\newblock
  \printfield[parens]{type}%
  \newunit\newblock
  \usebibmacro{location+date}%
  \newunit\newblock
  \iftoggle{bbx:url}
    {\usebibmacro{url+urldate}}
    {}%
  \newunit\newblock
  \usebibmacro{addendum+pubstate}%
  \setunit{\bibpagerefpunct}\newblock
  \usebibmacro{pageref}%
  \newunit\newblock
  \usebibmacro{related}%
  \usebibmacro{finentry}}
% Stop Latex from repositioning tables like an idiot
\restylefloat{table}

% Itemised columns
\makeatletter
\newcolumntype{i}[1]{%
    >{\minipage[t]{\linewidth}\let\\\tabularnewline
      \itemize
      \addtolength{\rightskip}{0pt plus 50pt}% for raggedright
      \setlength{\itemsep}{-\parsep}}%
    p{#1}%
    <{\@finalstrut\@arstrutbox\enditemize\endminipage}}
\makeatother


\begin{document}

\begin{center}
  \huge 3rd Year Group Project Final Report \\ [0.4cm]
  \large Sean Allan, Mitchell Allison, Sam Esgate, Tom Harling, Ted Sales,
         Max Tottenham \\ [0.2cm]
  \vspace{0cm}
\end{center}

\tableofcontents
\clearpage

\section{Executive Summary}
  Our project is, at its heart, a language translator. It translates a
  restricted version of first order predicate logic into the database querying
  language SQL. This allows us to open up new avenues in the realms of
  teaching, database administration, and computer security.

  \subsection*{Security}
  In the world of computer security, access to data is an important topic. One
  which has come to public attention through the recent leaks from NSA
  contracter Edward Snowden. Our product provides the bedrock for an
  information security system. 

  Imagine a scenario where your company needs to work with another large
  organisation, for example a telecommunications company working with the US
  Government. Your company is passionate about your customers privacy rights
  and so you want to give the government as little information as possible
  about your customers, whilst still allowing them to do their job. This could
  be as secretive as not even allowing the governement to know what kinds of
  data you hold. What you would like is a system which would allow the
  goverment to ask you a question, and for you to be able to respond with as
  little information as possible to answer that question without the government
  knowing what information you hold.

  Our product can provide the bedrock for this. Questions can be formed in
  predicate logic about whether a particular entity exists and if certain
  conditions surounding it hold, our project then takes this query and
  translates it into SQL, so that it is ready to be sent to a conventional
  database.

  \subsection*{Teaching}

  A compelling use for our project is for teaching, ideally suited for use 
  by students studying both logic and databases. To reinforce their 
  understanding of logic, queries can be written and tested against a suitably
  populated database of facts. The generated SQL can be used either to 
  highlight issues with the logic or, if correct, to explain the formulation
  of correct SQL.

  It also lends itself well to creating problem sheets for these subjects.
  For example, given a statement in english, like "What are the names of
  dragons that are happy?" what would be the logical expression and equivalent
  SQL query.

  \subsection*{Elevator Pitch (quite a hard sell)}
  \emph{>> What we have...}
  
  We can create SQL queries from logic statements.
  
  \emph{>> Why you want it...}

   It can be used to:
   \begin{itemize}
     \item query a database, without knowledge of SQL
     \item teach the basics of logic and SQL
   \end{itemize}

   \emph{>> Why is it useful...}
   \begin{itemize}
     \item It brings some of the power of logic to regular database queries,
     without the need to migrate to datalog.
    \end{itemize}

  What is your Project? What does it do? Why would I want to buy it? etc.
  No implementation, software engineering details, or project management

\section{Introduction}

  Set the scene ... motivation'
  State the problem you are trying to solve ...objective(s)
  Summarise your main achievements 

When we began this project our goal was simply to allow a database to be 
queried using logic. You shouldn't need to know the syntax of a particular
version of SQL, you shouldn't have to explicitly tell the DBMS how to extract
the data, all you need to do is write a query in logic and we will
get it for you.

We wanted to make it as easy as possible to write queries and view results;
our objective was to make it easy to understand, easy to write and to
provide clear advice when it does go wrong.

At the end of the project we can say that we have achieved most of our initial 
goals. We have created a system that can convert the queries, we have created
a simple way to construct and edit these queries, one that can be changed to 
work on any database and will provide clear error messages.

  -- GOALS - In Chatley coursework 1 --
  TODO: Discuss goals, they're referenced in the Project Management
  section. Revisions should not be talked about here though, just the
  overarching goals of the project.

\section{Theory}

  Structured Query Language (SQL)~\cite{wiki:SQL} is a special-purpose
  programming language designed for managing data held in relational database
  management system. SQL is made up of two different types of language:

  \begin{itemize}
      \item
        Data Definition Language. DDL is used to define the structure of a
        database (also known as its schema.
      \item
        Data Manipulation Language. DML is primarily used to insert, select,
        delete and update data within a database.
  \end{itemize}

  However, as defined by the SQL92 standard\cite{isoSQL}, read-only operations
  such as 'SELECT' (without 'INSERT INTO') should not exist as part of the DML;
  they do not manipulate the data, only query it. However, the distinction
  between read-only and read-write is not enforced. Here we will focus only on
  these operations (so called 'SQL-data' operations), with the aim of purely
  querying the RDBMS without altering it.

  \subsection{Tuple Relational Calculus}
    SQL was originally based on Relational Algebra and Tuple Relational
    Calculus. RA forms the structure and operations that can be performed
    across tuples of data and TRC provides a query language for such a model.

    \subsubsection{Formal Specification of Tuple Relational Calculus\cite{lecRA}}
      \label{sec:formalTRC}
      A query takes the form: \{T | p(T)\}

      The answer is the set of all tuples T such that p(T) evaluates to true.

      A formula F is recursively defined as:
      \begin{itemize}
        \item $p(t_1, ..., t_n)$ where predicate $p$ is applied to terms $t_1, ..., t_n$ (atomic formula)
        \item $\lnot F$ where $F$ is a formula.
        \item $F' \land F''$ where $F', F''$ are formulae.
        \item $F' \lor F''$ where $F', F''$ are formulae.
        \item $\exists t(F)$ where $F$ is a formula and $t$ is a tuple of terms.
        \item $\forall t(F)$ where $F$ is a formula and $t$ is a tuple of terms.
      \end{itemize}

      $\exists t(F)$ is true if, for some tuple $t$ the formula $F$ is true. \\
      $\forall t(F)$ is true if, for all tuples $t$ the formula $F$ is true.

      A variable $v$ is bound in $F$ if it is of the form:

      $\exists t(F)$ or $\forall t(F)$ and $v \in t$.

      Otherwise $v$ is said to be free.
      
      Note that in query \{T | p(T)\} the variables in T must be the only free variables in p(T).

    \subsubsection{Querying Relational Algebra}
      Using all of this, it is possible to begin formulating some simple
      queries. For example, suppose it was desirable to query all films from a
      particular director in a database. It would be simple to write:

      \{F | F $\in$ Films $\land$ F.director = "Matt Damon"\}

      This would however return all the elements of the tuple. The tuple may
      contain many irrelevant values to the user, and so it would be desirable
      to only return the title of said film. This could be achieved as detailed
      below:

      \{F | $\exists$F1 $\in$ Films(F1.director = "Matt Damon" $\land$ F.title =
      F1.title)\}

      We are now able to represent projection and selection. We can also
      represent joins. Let us suppose that we wish to find all films whose
      director has also directed another film.

      \{F | $\exists$F1 $\in$ Films($\exists$F2 $\in$ Films(Films1.director =
      F2.director $\land$ F1 != F2))\}

  \subsection{Mapping Tuple Relational Calculus to First-order Predicate Logic}
    With the knowledge of TRC, and how it underpins SQL, it is now necessary to
    formulate a mapping between TRC and first order predicate logic.

    \subsubsection{Atoms, Formulae, Predicates}
      Firstly atoms, formulae and predicates remain unchanged. We still wish to
      find a set of tuples that will result in a given formula evaluating to
      true.

    \subsubsection{Set Membership}
      Whereas before we could simply test the membership of a tuple $t$ in a
      relation $R$ with $t \in R$, first order predicate logic does not have
      the notion of set membership. From here there are a few different
      solutions to the problem.

      One solution is with an 'InRelation' predicate, where 'InRelation(t, R)'
      would have the same semantic meaning as the previous set membership test.
      This seems to work, although throughout first order predicate logic, we
      do not have the notion of a tuple; an atom is structurally meaningless. A
      disadvantage of this would be that accessing members of the tuple would
      require another predicate, for example 'InTuple(a, 'attrName', t)' which
      would have the same semantic meaning as 't.a' in TRC.

      Another solution, which solves the previous issue, is to generate n-ary
      predicates that represent a tuple. For example, given a relation 'films',
      with columns 'title', 'length' and 'date-released', a predicate
      'films(title, length, date-released)' could be generated, where title,
      length and date-released are all variables. This certainly solves the
      issue the set membership issue, but does add uneccessary bloat to each
      query; relations may have thousands of columns, very few of which are
      likely needed in a given query.

      The solution chosen addresses the issues raised so far.

      For a relation R(k, $a_{1}$, $a_{2}$, ..., $a_{n}$) with primary key k
      and n attributes, n + 1 predicates are generated as follows:
      \begin{enumerate}
        \item R(k)
        \item R\_$a_{1}$(k, a1)
        \item R\_$a_{2}$(k, a2)
        \\ \vdots
        \item[n + 1.] R\_$a_{n}$(k, an)
      \end{enumerate}

      This reduces the number of unncessary attributes in a given query, whilst
      also addressing the representation of a tuple, by relating a tuple to its
      primary key. This does however add the restriction that a primary key
      must exist on a relation for it to be represented in a query.

    \subsection{Free and Bound Variables}

      Looking back to Tuple Relational Calculus in
      section~\ref{sec:formalTRC}, we will use similar semantics for free and
      bound variables. A variable $v$ is said to be bound if formula $F$ is of
      the format:

      $\exists t(F)$ or $\forall t(F)$ and $v \in t$.

      However, in TRC, the syntax allowed for a return value, namely $t$ in the
      query below:

      \{T | p(T)\}

      Without extending predicate logic, we cannot use a similar format.
      It is important to note that in TRC, the variables that exist
      within $T$ must be the only free variables in $p(T)$. That is to say that
      TRC returns the set of all tuples that would result in $p(T) \models
      \top$ when substituted for the free variables. As $T$ must be the set of
      free variables $\in p(T)$, our predicate logic semantics for a query will
      assume that the answer will be the set of substitutions for all free
      variables in $p(T)$.

  \section{Translation of First-order Predicate Logic to SQL}

    Now that we have a syntactical map between first-order predicate logic and
    tuple relational calculus, we can begin to define the mapping between logic
    and SQL. First-order predicate logic
    \todo[inline]{Back this up. I believe it is true but how can it be proved?}
    is more expressive than SQL, and so there will not always exist a
    translation between logic and SQL. For this reason, this section will aim
    to describe common SQL queries or patterns in terms of their logical
    equivalence.

    The examples in the section below will make use of the filmdb as used in
    the project. A description of the SQL schema can be found in the appendix.

    \subsection{SQL SELECT}

    The SELECT statement is used to select data from a database.\cite{w3SELECT}
    Suppose that we wish to query the database for every film title in the
    database. Below is an example of an SQL query that satisfies this criteria.

    \begin{minted}{sql}

    SELECT title
    FROM films;

    \end{minted}

    This is equivalent to the following first-order predicate logic query.
    \begin{gather}
      \exists x(films\_title(x, title)) \label{select1}
    \end{gather}
    The query binds $x$ and allows $title$ to remain free, meaning that our
    answer will be the set of all subtitutions for $title$ that would result in
    $\exists x(films\_title(x, title)) \models \top$. The '$\exists$' symbol
    serves two purposes. Firstly it binds $x$ as mentioned above, and secondly
    it enforces that $title$ only need exist in one tuple, but may exist in
    more. The predicate $films\_title$ gives the translator the scope of
    relations to query (in this case, just the $films$ relation).

    Strictly talking, the logic query actually evaluates to the following, due
    to SQL's relational algebra foundations:

    \todo{I'm certain of this, but again evidence would be useful.}

    \begin{minted}{sql}

    SELECT DISTINCT title
    FROM films;

    \end{minted}

    \todo{Is sound the right term?}

    Note that it is also logically sound to express a query with no bound
    variables, such as:
    \begin{gather}
      films\_title(x, title) \label{select2}
    \end{gather}
    This will return the set of all tuples of the form \{$films.fid,
    films.title$\}, using the following SQL query.

    \begin{minted}{sql}

    SELECT fid, title,
    FROM films;

    \end{minted}

    Note that the DISTINCT keyword is no longer needed, as $films.fid$ is a
    primary key.

    Now we have demonstrated projecting a primary key, and a single attribute,
    all that is left is to project multiple attributes from a tuple. This
    requires the conjunction of multiple predicates, each over the same
    relation and using the same primary key (that is to say that each predicate
    relates to a single tuple). Suppose we wish to project
    every film title, and its corresponding length.

    \begin{gather}
      \exists x(films\_title(x, y) \land films\_length(x, z)
    \end{gather}

    As both $y$ and $z$ are from the same unique tuple identified by $x$
    (enforced by the conjunction within the query) our
    answer is equivalent to that returned by the following SQL query.

    \begin{minted}{sql}
    SELECT name, length
    FROM films;
    \end{minted}

    This can be extended to any number of predicates, although the relation and
    the key must be the same. If this is not the case, the query may represent
    a join, later described in section~\ref{sec:joins}.

  \subsection{SQL WHERE}

    The WHERE clause is used to extract only those records that fulfill a
    specified criterion.~\cite{w3WHERE} 

    \subsubsection{Equality}

      Suppose that we wish to check the existence of a film (The Bourne
      Identity) within our database. Below is an SQL query that satisfies this
      criteria.

      \begin{minted}{sql}

      SELECT title,
      FROM films
      WHERE title = 'The Bourne Identity';

      \end{minted}

      We expect to encounter at least one tuple if the film exists in our domain
      of discourse. \todo{Cite this from our AD lectures} This can be represented
      in one of two ways in first-order predicate logic.
      \begin{gather}
        \exists x(films\_title(x, y) \land y = \text{'The Bourne
        Identity'})\label{where1}\\
        films\_title(x, \text{'The Bourne Identity'})\label{where2}
      \end{gather}
      Both of these queries are valid, however note that they do return different
      tuples. (\ref{where1}) returns the same answer as the SQL query above, although
      it is verbose. (\ref{where2}) returns the singleton tuple \{$films.fid$\},
      because under our definition, only free variables are returned. Most of
      the time this is acceptable, as it is often undesirable to return a
      constant term in the results, and also has the benefit of being more
      compact.

    \subsubsection{Inequality}

      Suppose that we wish to check the inverse of the previous section, that
      there exists other films except 'The Bourne Identity'. In SQL, this
      simply translates to the following.

      \begin{minted}{sql}
        SELECT title,
        FROM films
        WHERE title <> 'The Bourne Identity';
      \end{minted}

      The existence of one or more tuples satisfies our query; if no queries
      are returned, it must be the case that there are no other films except
      'The Bourne Identity'.

      As before, there are two ways we can represent the SQL query above in
      logic.
      \begin{gather}
        \exists x(films\_title(x, y) \land y !=  \text{'The Bourne
        Identity'})\label{where3}\\
        \lnot films\_title(x, \text{'The Bourne Identity'})\label{where4}
      \end{gather}
      The first query matches the output of our equivalent SQL query, but again
      is more verbose. The second returns the set of singleton tuples
      \{$films.fid$\} such that the title associated with that particular
      primary key is not 'The Bourne Identity'. This is not equivalent to our
      SQL expression however, and may be undesirable to use in this
      circumstance (it is likely that we would like to return all other film
      titles).

      \todo{Do we use <= or $\le$}

      We can also express other inequalities, such as '<', '<=', '>', '>='.
      Here we query for all the fid of every films with length greater than or
      equal to two hours.

      \begin{minted}{sql}
      SELECT fid,
      FROM films,
      WHERE length >= '2:00:00';
      \end{minted}

      This can be expressed using the following predicate logic query.
      \begin{gather}
        \exists x(films\_length(x, y) \land x >= \text{'2:00:00'}) \label{where5}
      \end{gather}

      Similar queries can be constructed for the other inequalities in a
      similar format.

    \subsubsection{Logical Connectives}

      It is often desirable to project a set of attributes from a statement
      made up of conditions and logical connectives. For example, suppose we
      wished to query the database for all film titles where the film is between 
      two and three hours in length. The relevant SQL query would take one of
      the following forms.

      \begin{minted}{sql}
      SELECT title
      FROM films
      WHERE films.length >= '2:00:00'
        AND films.length <= '3:00:00';

      SELECT title
      FROM films
      WHERE films.length BETWEEN '2:00:00' AND '3:00:00';
      \end{minted}

      This can simply be represented in first-order predicate logic with the
      following query.

      \begin{gather}
        \exists x, length(films\_title(x, title) \land films\_length(x,
        length)\\
        \land length >= '2:00:00' \land length <= '3:00:00')
      \end{gather}

      Notice how the last two constraints in the logical query translate
      with very little effort. The same applies to constraints seperated with
      an $\lor$ which is equivalent to OR in SQL.

    \subsubsection{NULL Values}

      \todo[inline]{I imagine here would be a good place to talk about NULLs.
      Max, we should go over our specification for NULL.}

  \subsection{Joins}
    \label{sec:joins}

    An SQL JOIN clause is used to combine rows from two or more tables, based
    on a common field between them.~\cite{w3JOINS} Joins can enable incredibly
    powerful queries, especially in modern database systems where information
    is normalized (split across many relations) to minimise duplication.

    \subsubsection{SQL CROSS JOIN}

      A cross join is equivalent to the cartesian product of two sets. Suppose
      that we wished to write a query that returned every possible pair of
      film title and actor name in the database. In ANSI-89 SQL, we could use 
      the following query.

      \begin{minted}{sql}
      SELECT title, name
      FROM films, actors;
      \end{minted}

      Alternatively, in ANSI-92 SQL we can explicitly write:

      \begin{minted}{sql}
      SELECT title, name
      FROM films CROSS JOIN actors;
      \end{minted}

      We can think of this query as the conjunction of two disjoint relations;
      the relations do not share a key, or if they do we are not using them as
      part of the join.

      \begin{gather}
        \exists fid, aid(films\_title(fid, title) \land actors\_name(aid, name))
      \end{gather}

      The example used is unlikely to be a useful query on the database. Cross
      joins are rarely used in this format. Instead, they are often combined
      with a WHERE clause to extract useful information. For example, if we
      wished to query every actor that acted in the film 'The Bourne Identity',
      we could could use the following logic query.

      \begin{gather}
        \exists fid(films\_title(fid, \text{'The Bourne Identity'}) \\
        \land actors\_fid(aid, fid) \land actors\_name(aid, name))
      \end{gather}

      Which specifies that we only wish to return the set of all singleton
      tuples \{$name$\} such that the actor starred in 'The Bourne Identity'.
      With our knowledge of SQL CROSS JOIN, we can express this as the
      following SQL query.

      \begin{minted}{sql}
      SELECT name
      FROM films CROSS JOIN actors
      WHERE films.fid = actors.fid AND films.title = 'The Bourne Identity';
      \end{minted}

      So far we have not discussed SQL aliases. Aliases are required if joining
      two instances of the same relation. To demonstrate this, suppose that we
      wish to generate the set of all pairs of actors that have appeared in the
      same film. This is achieved with the following SQL query.

      \begin{minted}{sql}
      SELECT a1.name,
             a2.name
      FROM films CROSS JOIN actors AS a1
        CROSS JOIN actors AS a2
      WHERE films.fid = a1.fid AND films.fid = a2.fid 
        AND a1.name != a2.name
      \end{minted}

      Notice that we cannot simply refer to the relation $actors$ anymore, as
      there are two of them; without an alias, the identifier would be
      ambiguous. Below is a representation of the query in logic.

      \begin{multline}
        \exists fid, a1, a2(films(fid) \land actors\_fid(a1, fid) \land \\
        actors\_fid(a2, fid) \land actors\_name(a1, name1)\\
        \land actors\_name(a2, name2) \land name1 != name2) \label{largeCROSS}
      \end{multline}

    \subsubsection{SQL INNER JOIN}

      Introduced in ANSI-92 SQL, INNER JOIN allows for a more readable version of
      the above CROSS JOIN, WHERE combination. The logical query~\ref{largeCROSS}
      can be written using the SQL INNER JOIN syntax as:

      \begin{minted}{sql}
      SELECT a1.name,
             a2.name
      FROM films CROSS JOIN actors AS a1
        CROSS JOIN actors AS a2
      ON films.fid = a1.fid AND films.fid = a2.fid 
        AND a1.name != a2.name
      \end{minted}

      INNER JOIN is the same as JOIN as of ANSI-92 SQL, and it the existence of
      the ON clause is not necessary (making it a cross join).

    \subsubsection{Translating to the Correct Join}

      Fundamentally, joining two relations often occurs as the result of a
      conjunction of predicates when translating from first-order predicate
      logic. However, there are a few different circumstances which affect the
      type of join that must be used. Figure~\ref{fig:jointranslation} explains
      the different types of join that can result from a query.

      \begin{figure}
        \label{fig:jointranslation}
        \begin{tabular}{ | p{\dimexpr 0.22\linewidth-2\tabcolsep} |
          p{\dimexpr 0.22\linewidth-2\tabcolsep} |
          p{\dimexpr 0.15\linewidth-2\tabcolsep} |
          p{\dimexpr 0.41\linewidth-2\tabcolsep} | } \hline
        LHS & RHS & Join & Explanation \\
        \hline
        $rel\_attr1(k, a)$ & $rel\_attr2(k, b)$ & No Join & 
        Both keys and relations are equal. This conjunction involves projecting
        two elements from the same tuple, and so no join is necessary.\\ \hline
        $rel1\_attr1(k1, a)$ & $rel2\_attr2(k2, b)$ & CROSS JOIN &
        The keys are different and the relations may or may not be equal. Here
        we assume that a != b. As there are no common elements, a cross join
        must occur.\\ \hline
        $rel1\_attr1(k1, a)$ & $rel2\_attr2(k2, a)$ & JOIN ON rel1.attr1 =
        rel2.attr2 &
        The keys are different, but the attributes rel1.attr1 and rel2.attr2
        are equal, thus we must join on these elements. Note that aliases are
        used to avoid ambiguity between the two sets of attributes if they
        overlap. Note that this may be combined with the example below to
        create a more complex join.\\ \hline
        $rel1\_attr1(k, a)$ & $rel2\_attr2(k, b)$ & JOIN ON rel1.key =
        rel2.key &
        The keys are the same and we assume the attributes rel1.attr1 and rel2.attr2
        are different. We must join on the keys. Note that aliases are
        used to avoid ambiguity between the two sets of attributes if they
        overlap. Note that this may be combined with the above example to
        create a more complex join.\\ \hline
      \end{tabular}
        \caption{A table explaining the correct join to use for a logical query
        conjunction.}
    \end{figure}

\section{Design and Implementation}
  Anandha: Summarise key implementation details (how did you do it? what technology was
  used and why? what other technology was considered, but not used and why?
  Any technical challenges encountered and how addressed?  Any risks
  anticipated, and how mitigated

  It was decided that the project would be written in Python. This was due to
  the large number of plugins and frameworks available. Also since Python is
  interpreted there would be no large compile time which would speed up the rate
  of work.

  The decision was taken to create a website and not a standalone application.
  Doing so would ensure that users would always be using the most up to date
  code, since all of the logic to SQL conversion would be handled on the server
  and not on their own machines. (add more reasons if you can think of them...
  was making the GUI nice easier?)


  \todo[inline]{ List of Packages Used:\\

                 Backend\\
                 -------\\
                 Psycogp2   - Database Connection\\
                 PLY        - Parser Generator\\
                 Config Parser - parsing configuration files\\

                 Middleware\\
                 ----------\\
                 Rabbitmq   - Message Broker (Acts as a task Q)\\
                 Celery     - Sets up a worker to pull jobs off task Q\\
                 Web.py     - Web Server\\

                 Front End\\
                 ---------\\
                 ACE        - Text Editor\\
                 Bootstrap  - CSS \\

                 Testing\\
                 -------\\
                 nose       - Testing Framework\\
                 paste      - Test utilities for generating web requests\\}
                 

  \subsection{Front-end site}
    \subsubsection{Web server}
      The web.py framework (link to http://webpy.org/ ) was chosen to host the
      user facing site. Doing so allowed us to quickly set up a functioning site
      and concentrate on developing useful features, instead of spending a large
      amount of time setting up a more complicated web server.

      The JSON data format was used to transfer the logic input by the user to the
      server and the translated SQL to the user from the server. The format was
      chosen as it was very easy to parse by Javascript running in the user's
      browser and was well documented online due to its wide existing use.

    \subsubsection{Webpage plugins and libraries}
      The site made heavy use of jQuery, which greatly simplified the handling
      of JSON data sent between the user and the server.  

    \subsubsection{Graphical user interface}
      Ted did some UI magic, burnt some villages and then ended up with awesome
      syntax highlighting and auto-fill stuff.

  \subsection{Middleware plugins}
    \subsubsection{Celery and RabbitMQ}
      To allow multiple users to access and use the site simultaneously, each
      logic translation request had to be handled by a separate process, whilst
      multiple requests had to be queueable.

      To achieve this, the Celery (http://www.celeryproject.org/ ) asynchronous
      task queue and RabbitMQ (http://www.rabbitmq.com/ ) message broker were
      used. Each logic sentence to translate could be easily packaged into its
      own task and added to a queue to be translated.

      \begin{verbatim}
        Awesome example (that could do with syntax highlighting):
        Use celery in task.py
        import task as worker
        result = worker.addToProcessingQueue.delay(logic, web.schema)
        response = result.get()

        @celery.task
        def addToProcessingQueue(logic, schema):
      \end{verbatim}



    \subsection{Logic to SQL translation}
    lex.py, yacc.py

    \subsubsection{Parse the logic into a logic AST}
      How we defined the language grammar

    \subsubsection{Generate a symbol table based on the logic AST}

    \subsubsection{Run a pass of semantic analysis on the logic AST to check for errors}

    \subsubsection{Generate an IR based on the logic AST}

    \subsubsection{Generate an SQL string based on the above IR}
      Lots of scope to talk about each different construct (SELECT, SELECT FROM, JOIN, NATURAL JOIN etc.) and any optimisations we did.

  \subsection{Testing and continuous integration.. may want to move this section}
    nose tests, TeamCity
    \subsection{Backend}
     \subsubsection{Interfacing with the databse}

      We used several publicly available Python libraries in order to make 
      it very easy to query SQL databases. In particular we prioritised
      producing modular code so that our team members who were working on the
      other parts of the project could concentrate on their task, without having
      to worry about how the backend querying worked.

      In order to run an SQL query you have to run through the following steps:

      1. Grab some configuration (username password etc)
      2. Open a connection
      3. Run a query
      4. Close the connection.

      Just using psycopg2, the PostgreSQL adapter for Python, it would take
      several function calls to achieve each step. For example in order to run a
      query and return the results just using psycopg2 (assuming you already
      have a connection object setup) you would need to do the following:
      
     
      \begin{minted}{python}
      def query(con, query):
        result = {}
        try:
          cur = con.cursor()
          cur.execute(query)

          # Convert all rows to string representation
          result['rows'] = []
          for row in cur.fetchall():
            result_row = []
            for val in row:
              result_row.append(str(val))
            result['rows'].append(result_row)

          result['columns'] = [desc[0] for desc in cur.description]
          result['status'] = 'ok'
        except psycopg2.DatabaseError, e:
          result['error'] = str(e)
          result['status'] = 'db_error'
        finally:
          return result
      \end{minted}

      we decided to come up with the following architecture: 
      1. Creation of a configuration data structure. (either from a file or user
         entry)
      2. using that configuration data structure to create a connection object
      3. query the database by passing a function a connection object and a
      string containing the SQL query.
      4. close the connection object when all queries are finished

      This way, details about the database configuration and connection 
      can be passed around in an opaque manner. The final code for all four
      steps now looks like the following: 

      \begin{minted}{python}
        configData = cp.parse_file('dbbackend/db.cfg')
        con = pg.connect(configData)
        queryResult = pg.query(con, sql)
        con.close()
      \end{minted}

      \subsubsection{Codifying Database Schema}
      In an effort to make our project extensible, the web platform that performs 
      the logic to SQL translation uses an XML representation of the database
      schema. In this way the translation engine is platform agnostic with
      regards to which particular SQL implementation the web server is running.
      Indeed as long as an XML file is provided it is possible to run the
      translation engine in an 'offline' mode where it is not connected to a
      database and it does not run the translated query, instead it simply
      presents the query to the user.

      This XML representation is automatically generated on program startup. 
      Currently the project only supports PostgreSQL implementations, however it 
      has been designed so that support for other SQL implementations can be 
      added modularly. In order to extend support to other platforms all that is  
      required is to add a \%SQL\%\_backend.py which provides the functions
      described above, then switch out the import in generate\_schema.py to use
      the desired implementation. 




      (\todo{LOL} Max did you ever fix your multiple database issues?Also I think we're
      stuck with PostgreSQL databases only?) 
\section{Evaluation}
  Evaluate your deliverables e.g. performance, usability, etc.
  Summarise testing procedures + relevant testing results

\section{Conclusion and Future Extensions}
  What did you learn? What might you have done differently?
  How would you build on what you have done?

\section{Project Management}
  Planning, group organisation, breakdown + task allocation etc.

  \subsection{Planning}
    \todo[inline]{TODO Should we discuss here how everything went wrong, and
    revisions were missed entirely?}

    As we have learnt from previous projects, inside and outside the
    department, it is critical to thoroughly plan a software engineering
    exercise of this scale before beginning to implement features and write
    code. As discussed in our introduction section, we decomposed our problem
    of translating first order predicate logic to SQL into several key
    subgoals. This enabled us to outline a core feature set which we wished to
    implement and gave us a much clearer idea of how to begin tackling the
    problem.

    Moreover, we initially split the project up in to five major 'revisions'.
    Each revision added a certain amount of functionality to our project, and
    allowed us to continually build upon features already implemented. This
    incremental method of development is one of the key ideas of the Agile
    development methodology to which we adhered to throughout the project.
    \todo[inline]{TODO citation} Additionally, with each revision having a
    deadline, it gave the team a clear plan of what was to be implemented at
    what time.

    \begin{table}[H]
      \centering
      \begin{tabular}{| l | p{0.6\textwidth} | l |}
        \hline
        \textbf{Revision} & \textbf{Necessary Steps for Completion}
          & \textbf{Completion Estimate} \\
        % \multicolumn{1}{p{0.6\textwidth} |}{\textbf{}}
        \hline
        1 &
          \begin{compactitem}
            \item Set up basic web server;
            \item Set up and create a database;
            \item Create simple UI \& communicate with server;
            \item Create 5 sample Logic to SQL translations;
            \item Set up development environment.
          \end{compactitem}
          & 17/10/13 \\
        \hline
        2 &
          \begin{compactitem}
            \item UI sends predicate logic to web server;
            \item Web server parses the logic into an AST;
            \item Backend translates the AST to SQL for basic \texttt{SELECT
              FROM} queries (i.e. projection only);
            \item Create 5 more sample Logic to SQL translations.
          \end{compactitem}
          & 25/10/13 \\
        \hline
        3 &
          \begin{compactitem}
            \item Expand backend parser grammar to include more advanced use of
              SQL queries (selection \& projection);
            \item Dynamic table selection in logic - "smart" logic predicates,
              e.g. \texttt{updated(x)} vs. \texttt{customer\_updated(x)};
            \item Hook into database.
          \end{compactitem}
          & 1/11/13 \\
        \hline
        4 &
          \begin{compactitem}
            \item User-defined functions;
            \item Configurable UI - database settings, and possibly changing the
              UI depending on the user's ability;
            \item Extend backend to use SQL joins.
          \end{compactitem}
          & 8/11/13 \\
        \hline
        5 &
          \begin{compactitem}
            \item Contingency time built in for bug fixes, or additional
              features/extensions depending on the project status (e.g. Semantic
              checks of logical statements before translating to SQL).
          \end{compactitem}
          & 15/11/13 \\
        \hline
      \end{tabular}
      \caption{Feature sets for each revision, as defined at the start of the
        project}
    \end{table}

  \subsection{Group Organisation}
    At the beginning of the project, as discussed previously, we outlined the
    goals and required features of our project, and split them up in to three
    rough areas - front end (handling user interactions),
    middle end (communicating between client and server), and
    then the back end (translation and database access).
    We initially split our group of six in to three teams,
    corresponding to each of the subsections of our project. Members were
    allocated to each team depending on their strengths - initially Ted and Tom
    were to complete front end work, Sam and Mitchell to do middle end, and
    Sean and Max on back end. These were just initial roles, and we found that
    throughout the project, roles could be temporarily switched around to put
    extra work in to tasks that took longer than expected.

    One of the key ways we kept our group organised was through weekly
    meetings.  During each weekly meeting, we discussed what tasks were to be
    completed that week, and spoke in detail about any concerns members had
    about the tasks that had been completed previously. We would also analyse
    the previous weeks performance, and make changes to our plan accordingly.
    This falls in line with a key principle of Agile development, where a team
    must be adapatable to change. An example of such a change is when we
    realised implementing the parser would take significantly longer than
    originally thought. This lead us to modify our feature set, reallocate
    group roles temporarily, and modify deadlines and revision subgoals.

    \missingfigure{Group member Sam questioning the back-end architecture of
      the solution in one of our regular face-to-face meetings. (take pic from
      Chatley cwk 1)}

  \subsection{Tracking}
    Once an overarching plan was agreed on, we split each major goal of each
    revision in to smaller tasks. For each of these tasks, we then allocated a
    numerical estimate of the time it would take to complete, on a scale of 1
    to 5, where 1 signified a short task and 5 signified a very time consuming
    task. The estimate of time a task would take was initially based on
    previous experience on other software engineering projects the group
    participated in, but as the project progressed, we applied knowledge of how
    prior tasks went to more accurately assign estimates.

    A typical practice in Agile development is for developers to write down
    their tasks on cards, and then place them on a physical board (known as an
    "information radiator") located in sight of all developers on the project.
    Upon this board are "swim lanes"; columns which cards belong to, and denote
    their progress (e.g. "to do", "doing" and "done"). Each task has a time
    estimate (as discussed above), and is assigned to one or more people to
    complete.  This allows every developer to know exactly what is happening at
    what time, what has to be done, and what has been completed. For our group,
    working in DoC labs and at home, this would obviously not be practical.
    Therefore, we used web-based project management software called
    Trello. This software behaves in the same way as a physical board, but is
    accessed via a web based interface which can be edited by all members of
    the project.  Additionally, each card has a hashtag which denotes which
    revision a task belongs to, or if a stricter deadline is required, a date
    for completion can be manually added.  Virtual cards are moved between each
    swim lane as the task progresses. The group found this tool indispensible
    for communication and keeping on track with the project.

    \todo[inline]{TODO Citations for the above agile stuff}

    \missingfigure{Picture of our Trello board here. Do we want a screenshot
      from the first week, or towards the end? We have several versions as the
      project progressed in our Chatley reports.}

    -- Trello, Git integration

    -- Burndown chart

    \todo[inline]{TODO Talk about how our project adapated based on user
      feedback, version control (it might seem obvious that Git was used, but
      we should probably mention it along with a brief reason as to why we
      didn't use SVN/Mercurial or whatever). Continuous integration could be
      talked about here, but perhaps that belogs in the software development
      section? I'll see how the other sections are written first, then I'll put
      this stuff in; Anandha places importance on the report flowing, and I
      don't want it to feel disjointed.  Also, there should be a bit more
      discussion about Agile, but I don't know whether it belongs here or not.
      Maybe we should include some brief analysis on how our group found these
      techniques useful?}

\section{Bibliography}
  \printbibliography

  --TODO Agree on referencing practice. The library recommends Harvard. --

  https://workspace.imperial.ac.uk/library/Public/Harvard\_referencing.pdf

\section{Appendix}
  The appendix is optional, and does not count towards the 35 pages. It may
  contain thing like: User guide, installation instructions; more extensive
  design, testing, statistics etc.

  \todo{Insert the filmdb schema}

%\section{Anandha waffle from his site - delete once finished report}
%  Final Report ? due: 13th Jan 2014, at 16:00 (both Electronic and Hardcopy)
%
%  Contents for Final Report: The project report should not be longer than 35
%  pages (recommended length is around 30 pages), and might be organised
%  according to the following structure: see above sections
%
%  Make sure that the final report presents a coherent story. Ask advice from
%  your supervisor. You might also draw inspiration from the instructions about
%  writing up your individual project.
%
%  Bear in mind, that most of the project assessors will not have followed the
%  project throughout and will only have a short time to listen to a
%  presentation or see a demonstration. For this reason they will rely heavily
%  on the report to judge the project.
%
%  The report should be submitted to SGO in form of a hard copy, as well as
%  electronically through CATE. i.e. report.pdf. 
%
%  ------------------------
%
%  Assessment
%  Will be updated soon.
%
%  Group project
%  The group project assessment is undertaken by each group's supervisor, and
%  moderated by a larger group of assessors, who will attend your presentation,
%  and/or read your final report. The assessment is based on:
%
%      Executive Summary, 5
%      Presentation, 10
%      Group Collaboration and Management, 20
%      Report, 30
%      Technical Achievement, 35
%
%  The group project (along with the Software Engineering course) is worth 440
%  Marks for both the MEng and BEng students.
%  Overall assessment
%  The overall assessment is the sum of the group project component and the
%  Software Engineering Course in an 80:20 split.
%
\end{document}
